#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†æµ‹è¯•è„šæœ¬
æµ‹è¯•æ‰¹é‡è§£æåŠŸèƒ½çš„ä¿®å¤æ•ˆæœ
"""

import asyncio
import aiohttp
import json
import os
import time
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# APIåŸºç¡€URL
API_BASE = "http://localhost:8002"

class BatchProcessingTester:
    def __init__(self):
        self.session = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
            
    async def upload_test_files(self, num_files=10):
        """åˆ›å»ºå¹¶ä¸Šä¼ æµ‹è¯•æ–‡ä»¶"""
        test_files_dir = Path("test_batch_files")
        test_files_dir.mkdir(exist_ok=True)
        
        uploaded_docs = []
        
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        for i in range(1, num_files + 1):
            file_path = test_files_dir / f"test_doc{i}.txt"
            content = f"""æµ‹è¯•æ–‡æ¡£ {i}
            
è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯æ‰¹é‡å¤„ç†åŠŸèƒ½ã€‚
æ–‡æ¡£å†…å®¹åŒ…å«ï¼š
- æ–‡æ¡£æ ‡é¢˜ï¼šæµ‹è¯•æ–‡æ¡£{i}
- åˆ›å»ºæ—¶é—´ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}
- æµ‹è¯•ç›®çš„ï¼šéªŒè¯RAGç³»ç»Ÿæ‰¹é‡å¤„ç†èƒ½åŠ›

{'=' * 50}
å†…å®¹æ®µè½1ï¼šè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ®µè½ï¼ŒåŒ…å«ä¸€äº›åŸºæœ¬ä¿¡æ¯ã€‚
å†…å®¹æ®µè½2ï¼šè¿™é‡Œæ˜¯æ›´å¤šçš„æµ‹è¯•å†…å®¹ï¼Œç”¨äºRAGè§£æã€‚
å†…å®¹æ®µè½3ï¼šæœ€åä¸€ä¸ªæ®µè½ï¼Œå®Œæˆæ–‡æ¡£å†…å®¹ã€‚
{'=' * 50}

æ–‡æ¡£ç»“æŸæ ‡è®°ã€‚
"""
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            # ä¸Šä¼ æ–‡ä»¶
            logger.info(f"æ­£åœ¨ä¸Šä¼ æ–‡ä»¶: {file_path.name}")
            
            with open(file_path, 'rb') as f:
                data = aiohttp.FormData()
                data.add_field('file', f, filename=file_path.name)
                
                async with self.session.post(f"{API_BASE}/api/v1/documents/upload", data=data) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        uploaded_docs.append(result["document_id"])
                        logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {file_path.name} -> {result['document_id']}")
                    else:
                        error_text = await resp.text()
                        logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {file_path.name} - {error_text}")
                        
        return uploaded_docs
    
    async def start_batch_processing(self, document_ids):
        """å¯åŠ¨æ‰¹é‡å¤„ç†"""
        logger.info(f"ğŸš€ å¯åŠ¨æ‰¹é‡å¤„ç†ï¼Œæ–‡æ¡£æ•°é‡: {len(document_ids)}")
        
        payload = {
            "document_ids": document_ids
        }
        
        async with self.session.post(
            f"{API_BASE}/api/v1/documents/process/batch", 
            json=payload
        ) as resp:
            if resp.status == 200:
                result = await resp.json()
                logger.info(f"âœ… æ‰¹é‡å¤„ç†å¯åŠ¨æˆåŠŸ")
                logger.info(f"æ‰¹é‡æ“ä½œID: {result.get('batch_operation_id')}")
                logger.info(f"å¤„ç†ç»“æœç»Ÿè®¡: {result.get('summary')}")
                return result
            else:
                error_text = await resp.text()
                logger.error(f"âŒ æ‰¹é‡å¤„ç†å¯åŠ¨å¤±è´¥: {error_text}")
                return None
    
    async def monitor_processing_status(self, document_ids, timeout=300):
        """ç›‘æ§å¤„ç†çŠ¶æ€"""
        logger.info("ğŸ“Š å¼€å§‹ç›‘æ§å¤„ç†çŠ¶æ€...")
        
        start_time = time.time()
        check_interval = 5  # 5ç§’æ£€æŸ¥ä¸€æ¬¡
        
        while time.time() - start_time < timeout:
            # è·å–æ‰€æœ‰æ–‡æ¡£çŠ¶æ€
            status_counts = {
                "uploaded": 0,
                "queued": 0, 
                "processing": 0,
                "completed": 0,
                "failed": 0
            }
            
            for doc_id in document_ids:
                async with self.session.get(f"{API_BASE}/api/v1/documents/{doc_id}") as resp:
                    if resp.status == 200:
                        doc = await resp.json()
                        status = doc.get("status", "unknown")
                        if status in status_counts:
                            status_counts[status] += 1
                        else:
                            status_counts["unknown"] = status_counts.get("unknown", 0) + 1
            
            logger.info(f"ğŸ“ˆ çŠ¶æ€ç»Ÿè®¡: {status_counts}")
            
            # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å®Œæˆ
            total_finished = status_counts["completed"] + status_counts["failed"]
            if total_finished == len(document_ids):
                logger.info("ğŸ‰ æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆï¼")
                return status_counts
                
            await asyncio.sleep(check_interval)
        
        logger.warning("â° ç›‘æ§è¶…æ—¶ï¼Œä»æœ‰æ–‡æ¡£æœªå®Œæˆå¤„ç†")
        return status_counts
    
    async def get_batch_operation_status(self):
        """è·å–æ‰¹é‡æ“ä½œçŠ¶æ€"""
        async with self.session.get(f"{API_BASE}/api/v1/batch-operations") as resp:
            if resp.status == 200:
                operations = await resp.json()
                logger.info(f"ğŸ“‹ æ‰¹é‡æ“ä½œåˆ—è¡¨: {len(operations)} ä¸ªæ“ä½œ")
                for op in operations:
                    logger.info(f"  - {op.get('id')}: {op.get('status')} ({op.get('completed_count', 0)}/{op.get('total_count', 0)})")
                return operations
            else:
                logger.error("âŒ è·å–æ‰¹é‡æ“ä½œçŠ¶æ€å¤±è´¥")
                return []

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("ğŸ§ª å¼€å§‹æ‰¹é‡å¤„ç†æµ‹è¯•")
    
    async with BatchProcessingTester() as tester:
        try:
            # 1. ä¸Šä¼ æµ‹è¯•æ–‡ä»¶
            logger.info("ğŸ“ æ­¥éª¤1: ä¸Šä¼ æµ‹è¯•æ–‡ä»¶")
            document_ids = await tester.upload_test_files(10)  # ä¸Šä¼ 10ä¸ªæ–‡ä»¶
            
            if len(document_ids) < 5:
                logger.error("âŒ ä¸Šä¼ çš„æ–‡ä»¶æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæ‰¹é‡æµ‹è¯•")
                return
                
            logger.info(f"âœ… æˆåŠŸä¸Šä¼  {len(document_ids)} ä¸ªæ–‡ä»¶")
            
            # 2. å¯åŠ¨æ‰¹é‡å¤„ç†
            logger.info("âš™ï¸ æ­¥éª¤2: å¯åŠ¨æ‰¹é‡å¤„ç†")
            batch_result = await tester.start_batch_processing(document_ids)
            
            if not batch_result:
                logger.error("âŒ æ‰¹é‡å¤„ç†å¯åŠ¨å¤±è´¥")
                return
                
            # 3. ç›‘æ§å¤„ç†çŠ¶æ€
            logger.info("ğŸ‘€ æ­¥éª¤3: ç›‘æ§å¤„ç†çŠ¶æ€")
            final_status = await tester.monitor_processing_status(document_ids, timeout=600)
            
            # 4. è·å–æ‰¹é‡æ“ä½œçŠ¶æ€
            logger.info("ğŸ“Š æ­¥éª¤4: è·å–æ‰¹é‡æ“ä½œçŠ¶æ€")
            await tester.get_batch_operation_status()
            
            # 5. åˆ†æç»“æœ
            logger.info("ğŸ“‹ æ­¥éª¤5: åˆ†ææµ‹è¯•ç»“æœ")
            logger.info("=" * 60)
            logger.info("ğŸ æµ‹è¯•ç»“æœæ€»ç»“:")
            logger.info(f"æ€»æ–‡æ¡£æ•°: {len(document_ids)}")
            logger.info(f"æˆåŠŸå¤„ç†: {final_status.get('completed', 0)}")
            logger.info(f"å¤„ç†å¤±è´¥: {final_status.get('failed', 0)}")
            logger.info(f"ä»åœ¨é˜Ÿåˆ—: {final_status.get('queued', 0)}")
            logger.info(f"æ­£åœ¨å¤„ç†: {final_status.get('processing', 0)}")
            
            # åˆ¤æ–­æµ‹è¯•æ˜¯å¦æˆåŠŸ
            success_rate = final_status.get('completed', 0) / len(document_ids) * 100
            if success_rate >= 80:
                logger.info(f"âœ… æµ‹è¯•é€šè¿‡! æˆåŠŸç‡: {success_rate:.1f}%")
            else:
                logger.warning(f"âš ï¸ æµ‹è¯•å­˜åœ¨é—®é¢˜ï¼ŒæˆåŠŸç‡: {success_rate:.1f}%")
                
            logger.info("=" * 60)
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise

if __name__ == "__main__":
    asyncio.run(main())
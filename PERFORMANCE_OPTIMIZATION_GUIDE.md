# RAG-Anything API æ€§èƒ½ä¼˜åŒ–å®æ–½æŒ‡å—

## å¿«é€Ÿå¼€å§‹

### è¿è¡Œæ€§èƒ½æµ‹è¯•
```bash
# 1. ç¡®ä¿APIæœåŠ¡å™¨è¿è¡Œ
cd RAG-Anything/api
python rag_api_server.py

# 2. åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œæ€§èƒ½æµ‹è¯•
python run_performance_test.py
```

## ä¼˜åŒ–å®æ–½æ­¥éª¤

### ç¬¬ä¸€é˜¶æ®µï¼šç«‹å³ä¿®å¤ï¼ˆ1-3å¤©ï¼‰

#### 1. ä¿®å¤æ‰¹å¤„ç†ä¸²è¡Œé—®é¢˜ âš¡ é¢„æœŸæå‡ 300%

**é—®é¢˜ä½ç½®**: `RAG-Anything/raganything/batch.py` è¡Œ 350-358

**å½“å‰ä»£ç **:
```python
# ä¸²è¡Œå¤„ç†
for file_path in parse_result.successful_files:
    await self.process_document_complete(file_path, ...)
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# å¹¶è¡Œå¤„ç†
async def process_file_with_rag(file_path):
    try:
        return await self.process_document_complete(file_path, ...)
    except Exception as e:
        return {"error": str(e)}

# ä½¿ç”¨ gather å¹¶è¡Œæ‰§è¡Œ
tasks = [process_file_with_rag(fp) for fp in parse_result.successful_files]
results = await asyncio.gather(*tasks, return_exceptions=True)
```

**å®æ–½æ–‡ä»¶**: åˆ›å»º `RAG-Anything/api/optimize_batch_processing.py`

```python
#!/usr/bin/env python3
"""æ‰¹å¤„ç†ä¼˜åŒ–è¡¥ä¸"""

import asyncio
from typing import List, Dict, Any

async def optimized_batch_process(rag_instance, file_paths: List[str], **kwargs):
    """ä¼˜åŒ–çš„æ‰¹å¤„ç†å‡½æ•°"""
    
    # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘åº¦
    max_concurrent = kwargs.get('max_workers', 5)
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def process_with_limit(file_path: str):
        async with semaphore:
            return await rag_instance.process_document_complete(
                file_path, **kwargs
            )
    
    # å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶
    tasks = [process_with_limit(fp) for fp in file_paths]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    return results
```

#### 2. å®ç°å†…å­˜ç®¡ç† ğŸ§¹ é˜²æ­¢OOM

**åˆ›å»ºæ–‡ä»¶**: `RAG-Anything/api/memory_manager.py`

```python
#!/usr/bin/env python3
"""å†…å­˜ç®¡ç†å™¨"""

import gc
import time
from collections import OrderedDict
from typing import Any, Optional
import weakref

class MemoryManager:
    """å†…å­˜ç®¡ç†å™¨ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼"""
    
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache = OrderedDict()
        self.timestamps = {}
        self.weak_refs = weakref.WeakValueDictionary()
    
    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜é¡¹"""
        # æ£€æŸ¥å¤§å°é™åˆ¶
        if len(self.cache) >= self.max_size:
            # ç§»é™¤æœ€æ—§çš„é¡¹
            oldest = next(iter(self.cache))
            del self.cache[oldest]
            del self.timestamps[oldest]
        
        self.cache[key] = value
        self.timestamps[key] = time.time()
        
        # åˆ›å»ºå¼±å¼•ç”¨
        try:
            self.weak_refs[key] = value
        except TypeError:
            pass  # æŸäº›ç±»å‹ä¸æ”¯æŒå¼±å¼•ç”¨
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜é¡¹"""
        if key not in self.cache:
            return None
        
        # æ£€æŸ¥TTL
        if time.time() - self.timestamps[key] > self.ttl_seconds:
            del self.cache[key]
            del self.timestamps[key]
            return None
        
        # æ›´æ–°è®¿é—®é¡ºåº
        self.cache.move_to_end(key)
        return self.cache[key]
    
    def cleanup(self):
        """æ¸…ç†è¿‡æœŸé¡¹"""
        current_time = time.time()
        expired_keys = [
            k for k, t in self.timestamps.items()
            if current_time - t > self.ttl_seconds
        ]
        
        for key in expired_keys:
            del self.cache[key]
            del self.timestamps[key]
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
    
    def get_memory_usage(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import psutil
        process = psutil.Process()
        
        return {
            "cache_size": len(self.cache),
            "memory_mb": process.memory_info().rss / 1024 / 1024,
            "weak_refs": len(self.weak_refs)
        }

# å…¨å±€å†…å­˜ç®¡ç†å™¨
memory_manager = MemoryManager()
```

**é›†æˆåˆ°APIæœåŠ¡å™¨**:
```python
# åœ¨ rag_api_server.py ä¸­æ·»åŠ 
from memory_manager import memory_manager

# æ›¿æ¢å…¨å±€å­—å…¸
tasks = memory_manager  # ä½¿ç”¨å†…å­˜ç®¡ç†å™¨æ›¿ä»£æ™®é€šå­—å…¸
documents = memory_manager

# æ·»åŠ å®šæœŸæ¸…ç†ä»»åŠ¡
async def periodic_cleanup():
    while True:
        await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
        memory_manager.cleanup()
        logger.info(f"å†…å­˜æ¸…ç†å®Œæˆ: {memory_manager.get_memory_usage()}")

# åœ¨ startup_event ä¸­å¯åŠ¨
asyncio.create_task(periodic_cleanup())
```

#### 3. ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ ğŸ’¾ æå‡å‘½ä¸­ç‡è‡³60%

**åˆ›å»ºæ–‡ä»¶**: `RAG-Anything/api/content_cache.py`

```python
#!/usr/bin/env python3
"""åŸºäºå†…å®¹çš„ç¼“å­˜ç³»ç»Ÿ"""

import hashlib
import json
from typing import Any, Dict, Optional
from pathlib import Path

class ContentBasedCache:
    """åŸºäºå†…å®¹å“ˆå¸Œçš„ç¼“å­˜"""
    
    def __init__(self, cache_dir: str = "/tmp/rag_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.stats = {"hits": 0, "misses": 0}
    
    def _generate_content_hash(self, file_path: str) -> str:
        """ç”Ÿæˆæ–‡ä»¶å†…å®¹å“ˆå¸Œ"""
        hasher = hashlib.sha256()
        
        with open(file_path, 'rb') as f:
            # è¯»å–æ–‡ä»¶å†…å®¹ç”Ÿæˆå“ˆå¸Œ
            while chunk := f.read(8192):
                hasher.update(chunk)
        
        return hasher.hexdigest()
    
    def get_cache_key(self, file_path: str, **config) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content_hash = self._generate_content_hash(file_path)
        
        # åªåŒ…å«å½±å“è§£æç»“æœçš„é…ç½®
        relevant_config = {
            k: v for k, v in config.items()
            if k in ['parser', 'parse_method', 'lang']
        }
        
        config_str = json.dumps(relevant_config, sort_keys=True)
        config_hash = hashlib.md5(config_str.encode()).hexdigest()
        
        return f"{content_hash}_{config_hash}"
    
    def get(self, file_path: str, **config) -> Optional[Any]:
        """è·å–ç¼“å­˜å†…å®¹"""
        cache_key = self.get_cache_key(file_path, **config)
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        if cache_file.exists():
            self.stats["hits"] += 1
            with open(cache_file, 'r') as f:
                return json.load(f)
        
        self.stats["misses"] += 1
        return None
    
    def set(self, file_path: str, content: Any, **config):
        """è®¾ç½®ç¼“å­˜å†…å®¹"""
        cache_key = self.get_cache_key(file_path, **config)
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        with open(cache_file, 'w') as f:
            json.dump(content, f)
    
    def get_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.stats["hits"] + self.stats["misses"]
        if total == 0:
            return 0.0
        return self.stats["hits"] / total * 100

# å…¨å±€ç¼“å­˜å®ä¾‹
content_cache = ContentBasedCache()
```

### ç¬¬äºŒé˜¶æ®µï¼šçŸ­æœŸä¼˜åŒ–ï¼ˆ1-2å‘¨ï¼‰

#### 4. å®ç°ä»»åŠ¡é˜Ÿåˆ—ç³»ç»Ÿ

**å®‰è£…ä¾èµ–**:
```bash
pip install celery redis
```

**åˆ›å»ºæ–‡ä»¶**: `RAG-Anything/api/task_queue.py`

```python
#!/usr/bin/env python3
"""Celeryä»»åŠ¡é˜Ÿåˆ—é…ç½®"""

from celery import Celery
import os

# åˆ›å»ºCeleryåº”ç”¨
app = Celery(
    'rag_tasks',
    broker=os.getenv('REDIS_URL', 'redis://localhost:6379/0'),
    backend=os.getenv('REDIS_URL', 'redis://localhost:6379/0')
)

# é…ç½®
app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    # ä»»åŠ¡è·¯ç”±
    task_routes={
        'rag_tasks.process_document': {'queue': 'high'},
        'rag_tasks.batch_process': {'queue': 'batch'},
        'rag_tasks.query': {'queue': 'query'}
    },
    # å¹¶å‘é…ç½®
    worker_concurrency=4,
    worker_prefetch_multiplier=2,
    # ä»»åŠ¡æ—¶é™
    task_soft_time_limit=300,
    task_time_limit=600,
)

@app.task(bind=True, max_retries=3)
def process_document_task(self, file_path: str, **kwargs):
    """å¼‚æ­¥å¤„ç†æ–‡æ¡£ä»»åŠ¡"""
    try:
        # å¤„ç†é€»è¾‘
        result = process_document_sync(file_path, **kwargs)
        return result
    except Exception as e:
        # é‡è¯•æœºåˆ¶
        raise self.retry(exc=e, countdown=60)

# å¯åŠ¨Worker
# celery -A task_queue worker --loglevel=info --queues=high,batch,query
```

#### 5. å®ç°å¼‚æ­¥I/O

**åˆ›å»ºæ–‡ä»¶**: `RAG-Anything/api/async_io_utils.py`

```python
#!/usr/bin/env python3
"""å¼‚æ­¥I/Oå·¥å…·"""

import aiofiles
import asyncio
from pathlib import Path
from typing import AsyncIterator, Optional

async def async_read_file(file_path: str, chunk_size: int = 8192) -> bytes:
    """å¼‚æ­¥è¯»å–æ–‡ä»¶"""
    async with aiofiles.open(file_path, 'rb') as f:
        return await f.read()

async def async_write_file(file_path: str, content: bytes):
    """å¼‚æ­¥å†™å…¥æ–‡ä»¶"""
    async with aiofiles.open(file_path, 'wb') as f:
        await f.write(content)

async def async_read_chunks(
    file_path: str, 
    chunk_size: int = 1024 * 1024
) -> AsyncIterator[bytes]:
    """å¼‚æ­¥åˆ†å—è¯»å–å¤§æ–‡ä»¶"""
    async with aiofiles.open(file_path, 'rb') as f:
        while chunk := await f.read(chunk_size):
            yield chunk

class AsyncFilePool:
    """å¼‚æ­¥æ–‡ä»¶æ“ä½œæ± """
    
    def __init__(self, max_concurrent: int = 10):
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def process_files(self, file_paths: list, processor):
        """å¹¶å‘å¤„ç†å¤šä¸ªæ–‡ä»¶"""
        async def process_with_limit(file_path):
            async with self.semaphore:
                return await processor(file_path)
        
        tasks = [process_with_limit(fp) for fp in file_paths]
        return await asyncio.gather(*tasks, return_exceptions=True)
```

#### 6. GPUå†…å­˜ç®¡ç†

**åˆ›å»ºæ–‡ä»¶**: `RAG-Anything/api/gpu_manager.py`

```python
#!/usr/bin/env python3
"""GPUå†…å­˜ç®¡ç†å™¨"""

import torch
import gc
from typing import Optional

class GPUMemoryManager:
    """GPUå†…å­˜ç®¡ç†å™¨"""
    
    def __init__(self, device: str = "cuda"):
        self.device = device
        self.max_memory_mb = self._get_max_memory()
        self.fallback_to_cpu = True
    
    def _get_max_memory(self) -> float:
        """è·å–GPUæœ€å¤§å†…å­˜"""
        if not torch.cuda.is_available():
            return 0
        
        return torch.cuda.get_device_properties(0).total_memory / 1024 / 1024
    
    def get_available_memory(self) -> float:
        """è·å–å¯ç”¨GPUå†…å­˜"""
        if not torch.cuda.is_available():
            return 0
        
        return (torch.cuda.get_device_properties(0).total_memory - 
                torch.cuda.memory_allocated()) / 1024 / 1024
    
    def clear_cache(self):
        """æ¸…ç†GPUç¼“å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
    
    def adaptive_batch_size(self, base_batch_size: int = 8) -> int:
        """æ ¹æ®å¯ç”¨å†…å­˜è‡ªé€‚åº”è°ƒæ•´æ‰¹å¤§å°"""
        available = self.get_available_memory()
        
        if available < 1000:  # < 1GB
            return max(1, base_batch_size // 4)
        elif available < 2000:  # < 2GB
            return max(2, base_batch_size // 2)
        else:
            return base_batch_size
    
    def safe_gpu_operation(self, func, *args, **kwargs):
        """å®‰å…¨çš„GPUæ“ä½œï¼ŒOOMæ—¶è‡ªåŠ¨é™çº§åˆ°CPU"""
        try:
            return func(*args, **kwargs)
        except torch.cuda.OutOfMemoryError:
            if self.fallback_to_cpu:
                self.clear_cache()
                # åˆ‡æ¢åˆ°CPU
                kwargs['device'] = 'cpu'
                return func(*args, **kwargs)
            raise

# å…¨å±€GPUç®¡ç†å™¨
gpu_manager = GPUMemoryManager()
```

### ç¬¬ä¸‰é˜¶æ®µï¼šç›‘æ§å’ŒæŒç»­ä¼˜åŒ–

#### 7. æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿

**åˆ›å»ºæ–‡ä»¶**: `RAG-Anything/api/monitoring_dashboard.py`

```python
#!/usr/bin/env python3
"""æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿"""

from flask import Flask, render_template_string
import json
import psutil
from datetime import datetime

app = Flask(__name__)

DASHBOARD_HTML = """
<!DOCTYPE html>
<html>
<head>
    <title>RAG-Anything Performance Dashboard</title>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <style>
        body { font-family: Arial; margin: 20px; background: #f5f5f5; }
        .metric { 
            display: inline-block; 
            margin: 10px; 
            padding: 15px; 
            background: white; 
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .metric-value { font-size: 32px; font-weight: bold; }
        .metric-label { color: #666; margin-top: 5px; }
        #charts { margin-top: 20px; }
        .chart { 
            background: white; 
            padding: 20px; 
            margin: 10px 0;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
    </style>
</head>
<body>
    <h1>ğŸš€ RAG-Anything Performance Dashboard</h1>
    
    <div id="metrics">
        <div class="metric">
            <div class="metric-value" id="cpu">{{ cpu }}%</div>
            <div class="metric-label">CPU Usage</div>
        </div>
        <div class="metric">
            <div class="metric-value" id="memory">{{ memory }}%</div>
            <div class="metric-label">Memory Usage</div>
        </div>
        <div class="metric">
            <div class="metric-value" id="throughput">{{ throughput }}</div>
            <div class="metric-label">Docs/Min</div>
        </div>
        <div class="metric">
            <div class="metric-value" id="cache">{{ cache_hit }}%</div>
            <div class="metric-label">Cache Hit Rate</div>
        </div>
    </div>
    
    <div id="charts">
        <div class="chart">
            <div id="cpu-chart"></div>
        </div>
        <div class="chart">
            <div id="throughput-chart"></div>
        </div>
    </div>
    
    <script>
        // å®æ—¶æ›´æ–°
        setInterval(function() {
            fetch('/api/metrics')
                .then(response => response.json())
                .then(data => {
                    document.getElementById('cpu').innerText = data.cpu + '%';
                    document.getElementById('memory').innerText = data.memory + '%';
                    document.getElementById('throughput').innerText = data.throughput;
                    document.getElementById('cache').innerText = data.cache_hit + '%';
                    
                    // æ›´æ–°å›¾è¡¨
                    updateCharts(data);
                });
        }, 2000);
        
        function updateCharts(data) {
            // CPUè¶‹åŠ¿å›¾
            Plotly.newPlot('cpu-chart', [{
                y: data.cpu_history,
                type: 'line',
                name: 'CPU Usage'
            }], {
                title: 'CPU Usage Trend',
                yaxis: { title: 'Usage %' }
            });
            
            // ååé‡å›¾
            Plotly.newPlot('throughput-chart', [{
                y: data.throughput_history,
                type: 'line',
                name: 'Throughput'
            }], {
                title: 'Processing Throughput',
                yaxis: { title: 'Docs/Min' }
            });
        }
    </script>
</body>
</html>
"""

@app.route('/')
def dashboard():
    """ä»ªè¡¨æ¿é¡µé¢"""
    metrics = get_current_metrics()
    return render_template_string(DASHBOARD_HTML, **metrics)

@app.route('/api/metrics')
def metrics_api():
    """æŒ‡æ ‡API"""
    return json.dumps(get_current_metrics())

def get_current_metrics():
    """è·å–å½“å‰æŒ‡æ ‡"""
    return {
        "cpu": psutil.cpu_percent(interval=1),
        "memory": psutil.virtual_memory().percent,
        "throughput": 45,  # ä»å®é™…ç³»ç»Ÿè·å–
        "cache_hit": 65,   # ä»å®é™…ç³»ç»Ÿè·å–
        "cpu_history": [20, 25, 30, 35, 40, 38, 35, 32],
        "throughput_history": [40, 42, 45, 48, 50, 47, 45, 45]
    }

if __name__ == '__main__':
    app.run(debug=True, port=5000)
```

## éƒ¨ç½²ä¼˜åŒ–é…ç½®

### ç¯å¢ƒå˜é‡ä¼˜åŒ– (.env.performance)

```bash
# å¹¶å‘é…ç½®
MAX_CONCURRENT_PROCESSING=8
MAX_CONCURRENT_FILES=10
WORKER_POOL_SIZE=4

# ç¼“å­˜é…ç½®
ENABLE_PARSE_CACHE=true
ENABLE_LLM_CACHE=true
CACHE_SIZE_LIMIT=5000
CACHE_TTL_HOURS=72
CACHE_STORAGE=/data/rag_cache

# å†…å­˜ç®¡ç†
MAX_MEMORY_MB=8192
MEMORY_CLEANUP_INTERVAL=300
GC_THRESHOLD=1000

# GPUé…ç½®
CUDA_VISIBLE_DEVICES=0,1
GPU_MEMORY_FRACTION=0.8
FALLBACK_TO_CPU=true

# æ‰¹å¤„ç†ä¼˜åŒ–
BATCH_SIZE=20
BATCH_TIMEOUT=600
ENABLE_BATCH_CACHE=true

# ç›‘æ§
ENABLE_MONITORING=true
METRICS_PORT=9090
PROFILING_ENABLED=false
```

### Nginx é…ç½®ä¼˜åŒ–

```nginx
# /etc/nginx/sites-available/rag-api
upstream rag_backend {
    least_conn;
    server 127.0.0.1:8001 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:8002 max_fails=3 fail_timeout=30s;
    keepalive 32;
}

server {
    listen 80;
    server_name api.rag-anything.local;
    
    # ä¼˜åŒ–ç¼“å†²åŒº
    client_body_buffer_size 128k;
    client_max_body_size 100M;
    client_body_timeout 300;
    
    # å¯ç”¨å‹ç¼©
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain application/json;
    
    # ç¼“å­˜é™æ€å†…å®¹
    location ~* \.(jpg|jpeg|png|gif|ico|css|js)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
    
    # APIä»£ç†
    location /api/ {
        proxy_pass http://rag_backend;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        
        # è¶…æ—¶é…ç½®
        proxy_connect_timeout 60s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
        
        # ç¼“å†²é…ç½®
        proxy_buffering on;
        proxy_buffer_size 4k;
        proxy_buffers 8 4k;
        proxy_busy_buffers_size 8k;
    }
    
    # WebSocketæ”¯æŒ
    location /ws/ {
        proxy_pass http://rag_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_read_timeout 86400;
    }
}
```

### Systemd æœåŠ¡é…ç½®

```ini
# /etc/systemd/system/rag-api.service
[Unit]
Description=RAG-Anything API Server
After=network.target

[Service]
Type=simple
User=ragsvr
WorkingDirectory=/home/ragsvr/projects/ragsystem/RAG-Anything/api
Environment="PYTHONPATH=/home/ragsvr/projects/ragsystem/RAG-Anything"
ExecStart=/usr/bin/python3 rag_api_server.py
Restart=always
RestartSec=10

# èµ„æºé™åˆ¶
LimitNOFILE=65536
LimitNPROC=4096
MemoryLimit=8G
CPUQuota=400%

# æ€§èƒ½ä¼˜åŒ–
Nice=-5
IOSchedulingClass=best-effort
IOSchedulingPriority=0

[Install]
WantedBy=multi-user.target
```

## éªŒè¯ä¼˜åŒ–æ•ˆæœ

### è¿è¡Œå¯¹æ¯”æµ‹è¯•

```bash
# ä¼˜åŒ–å‰åŸºå‡†æµ‹è¯•
python run_performance_test.py > before_optimization.txt

# åº”ç”¨ä¼˜åŒ–
./apply_optimizations.sh

# ä¼˜åŒ–åæµ‹è¯•
python run_performance_test.py > after_optimization.txt

# ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
python compare_results.py before_optimization.txt after_optimization.txt
```

### é¢„æœŸæ”¹è¿›

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æ”¹è¿› |
|-----|--------|--------|------|
| æ‰¹å¤„ç†ååé‡ | 20 docs/min | 100 docs/min | +400% |
| ç¼“å­˜å‘½ä¸­ç‡ | 23% | 65% | +183% |
| CPUåˆ©ç”¨ç‡ | 25% | 70% | +180% |
| å†…å­˜ç¨³å®šæ€§ | æ³„æ¼ | ç¨³å®š | âœ“ |
| å¹¶å‘å¤„ç† | 3 | 20 | +567% |
| P95å“åº”æ—¶é—´ | 45s | 10s | -78% |

## é•¿æœŸä¼˜åŒ–è·¯çº¿å›¾

### Q1 2025
- [ ] å®ç°åˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—
- [ ] æ·»åŠ Redisç¼“å­˜å±‚
- [ ] GPUé›†ç¾¤æ”¯æŒ

### Q2 2025
- [ ] å¾®æœåŠ¡æ¶æ„æ”¹é€ 
- [ ] Kuberneteséƒ¨ç½²
- [ ] è‡ªåŠ¨æ‰©ç¼©å®¹

### Q3 2025
- [ ] è¾¹ç¼˜è®¡ç®—æ”¯æŒ
- [ ] å®æ—¶æµå¤„ç†
- [ ] AIä¼˜åŒ–è°ƒåº¦

## æ€»ç»“

é€šè¿‡å®æ–½è¿™äº›ä¼˜åŒ–ï¼ŒRAG-Anything APIçš„æ€§èƒ½å°†è·å¾—æ˜¾è‘—æå‡ï¼š

1. **ç«‹å³è§æ•ˆ**: æ‰¹å¤„ç†å¹¶è¡ŒåŒ–å¸¦æ¥ 3-5å€ æ€§èƒ½æå‡
2. **çŸ­æœŸæ”¹è¿›**: ç¼“å­˜å’Œå†…å­˜ä¼˜åŒ–æå‡ç¨³å®šæ€§
3. **é•¿æœŸæ¼”è¿›**: æ¶æ„å‡çº§æ”¯æŒå¤§è§„æ¨¡éƒ¨ç½²

å»ºè®®æŒ‰ä¼˜å…ˆçº§é€æ­¥å®æ–½ï¼Œæ¯ä¸ªé˜¶æ®µéªŒè¯æ•ˆæœåå†è¿›è¡Œä¸‹ä¸€æ­¥ä¼˜åŒ–ã€‚
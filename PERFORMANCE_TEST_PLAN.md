# RAG-Anything API æ€§èƒ½æµ‹è¯•æ–¹æ¡ˆ

## æµ‹è¯•ç›®æ ‡

1. **åŸºå‡†æ€§èƒ½æµ‹è¯•**: å»ºç«‹å½“å‰ç³»ç»Ÿæ€§èƒ½åŸºçº¿
2. **è´Ÿè½½æµ‹è¯•**: ç¡®å®šç³»ç»Ÿæœ€å¤§å¤„ç†èƒ½åŠ›
3. **å‹åŠ›æµ‹è¯•**: æ‰¾å‡ºç³»ç»Ÿå´©æºƒç‚¹
4. **å¹¶å‘æµ‹è¯•**: éªŒè¯å¹¶å‘å¤„ç†èƒ½åŠ›
5. **æŒä¹…æ€§æµ‹è¯•**: æ£€æµ‹å†…å­˜æ³„æ¼å’Œæ€§èƒ½é€€åŒ–

## 1. æµ‹è¯•ç¯å¢ƒå‡†å¤‡

### 1.1 ç¡¬ä»¶è¦æ±‚
```yaml
æµ‹è¯•æœåŠ¡å™¨:
  CPU: 8æ ¸å¿ƒä»¥ä¸Š
  å†…å­˜: 16GBä»¥ä¸Š
  GPU: NVIDIA GPU (å¯é€‰ï¼Œç”¨äºGPUæµ‹è¯•)
  å­˜å‚¨: SSDï¼Œ100GBå¯ç”¨ç©ºé—´

è´Ÿè½½ç”Ÿæˆå™¨:
  CPU: 4æ ¸å¿ƒ
  å†…å­˜: 8GB
  ç½‘ç»œ: ä¸æµ‹è¯•æœåŠ¡å™¨åŒä¸€å±€åŸŸç½‘
```

### 1.2 è½¯ä»¶ç¯å¢ƒ
```bash
# å®‰è£…æ€§èƒ½æµ‹è¯•å·¥å…·
pip install locust pytest-benchmark pytest-asyncio memory_profiler
pip install gputil psutil aiohttp

# å®‰è£…ç›‘æ§å·¥å…·
sudo apt-get install htop iotop nethogs
pip install prometheus-client grafana-api

# GPUç›‘æ§ï¼ˆå¦‚æœæœ‰GPUï¼‰
pip install nvidia-ml-py3
```

### 1.3 æµ‹è¯•æ•°æ®å‡†å¤‡
```python
# ç”Ÿæˆä¸åŒå¤§å°çš„æµ‹è¯•æ–‡æ¡£
æµ‹è¯•æ–‡æ¡£é›†:
  - å°æ–‡æ¡£: 100ä¸ª x 100KB (PDF/DOCX/TXT)
  - ä¸­æ–‡æ¡£: 50ä¸ª x 1MB
  - å¤§æ–‡æ¡£: 20ä¸ª x 10MB
  - æ··åˆç±»å‹: PDF(40%), DOCX(30%), TXT(20%), å…¶ä»–(10%)
```

## 2. æ€§èƒ½æµ‹è¯•è„šæœ¬

### 2.1 åŸºå‡†æ€§èƒ½æµ‹è¯•è„šæœ¬

åˆ›å»ºæ–‡ä»¶ `performance_tests/benchmark_test.py`:

```python
#!/usr/bin/env python3
"""
åŸºå‡†æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•å•æ–‡æ¡£å¤„ç†æ€§èƒ½å’Œæ‰¹å¤„ç†æ€§èƒ½
"""

import asyncio
import time
import statistics
import json
from pathlib import Path
from typing import List, Dict, Any
import aiohttp
import psutil
import GPUtil

class BenchmarkTest:
    def __init__(self, api_url: str = "http://127.0.0.1:8001"):
        self.api_url = api_url
        self.results = {
            "single_document": [],
            "batch_processing": [],
            "cache_performance": [],
            "resource_usage": []
        }
    
    async def test_single_document(self, file_path: str) -> Dict[str, Any]:
        """æµ‹è¯•å•æ–‡æ¡£å¤„ç†æ€§èƒ½"""
        start_time = time.time()
        
        # è®°å½•åˆå§‹èµ„æºä½¿ç”¨
        initial_cpu = psutil.cpu_percent(interval=1)
        initial_memory = psutil.virtual_memory().percent
        
        async with aiohttp.ClientSession() as session:
            # ä¸Šä¼ æ–‡æ¡£
            with open(file_path, 'rb') as f:
                data = aiohttp.FormData()
                data.add_field('file', f, filename=Path(file_path).name)
                
                upload_start = time.time()
                async with session.post(f"{self.api_url}/api/v1/documents/upload", data=data) as resp:
                    upload_result = await resp.json()
                upload_time = time.time() - upload_start
            
            document_id = upload_result.get('document_id')
            
            # è§¦å‘å¤„ç†
            process_start = time.time()
            async with session.post(f"{self.api_url}/api/v1/documents/{document_id}/process") as resp:
                process_result = await resp.json()
            
            task_id = process_result.get('task_id')
            
            # ç­‰å¾…å¤„ç†å®Œæˆ
            processing_complete = False
            poll_count = 0
            while not processing_complete and poll_count < 300:  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
                await asyncio.sleep(1)
                async with session.get(f"{self.api_url}/api/v1/tasks/{task_id}") as resp:
                    task_status = await resp.json()
                    if task_status['task']['status'] in ['completed', 'failed']:
                        processing_complete = True
                poll_count += 1
            
            process_time = time.time() - process_start
        
        # è®°å½•èµ„æºä½¿ç”¨
        final_cpu = psutil.cpu_percent(interval=1)
        final_memory = psutil.virtual_memory().percent
        
        total_time = time.time() - start_time
        
        result = {
            "file_path": file_path,
            "file_size": Path(file_path).stat().st_size,
            "upload_time": upload_time,
            "process_time": process_time,
            "total_time": total_time,
            "cpu_usage": final_cpu - initial_cpu,
            "memory_usage": final_memory - initial_memory,
            "success": task_status['task']['status'] == 'completed'
        }
        
        self.results["single_document"].append(result)
        return result
    
    async def test_batch_processing(self, file_paths: List[str], batch_size: int = 10) -> Dict[str, Any]:
        """æµ‹è¯•æ‰¹é‡å¤„ç†æ€§èƒ½"""
        start_time = time.time()
        
        async with aiohttp.ClientSession() as session:
            # æ‰¹é‡ä¸Šä¼ 
            upload_start = time.time()
            document_ids = []
            
            for file_path in file_paths[:batch_size]:
                with open(file_path, 'rb') as f:
                    data = aiohttp.FormData()
                    data.add_field('file', f, filename=Path(file_path).name)
                    
                    async with session.post(f"{self.api_url}/api/v1/documents/upload", data=data) as resp:
                        result = await resp.json()
                        document_ids.append(result['document_id'])
            
            upload_time = time.time() - upload_start
            
            # æ‰¹é‡å¤„ç†
            process_start = time.time()
            batch_data = {
                "document_ids": document_ids,
                "parser": "auto",
                "parse_method": "auto"
            }
            
            async with session.post(
                f"{self.api_url}/api/v1/documents/process/batch",
                json=batch_data
            ) as resp:
                batch_result = await resp.json()
            
            batch_operation_id = batch_result.get('batch_operation_id')
            
            # ç­‰å¾…æ‰¹å¤„ç†å®Œæˆ
            processing_complete = False
            poll_count = 0
            while not processing_complete and poll_count < 600:  # æœ€å¤šç­‰å¾…10åˆ†é’Ÿ
                await asyncio.sleep(2)
                async with session.get(f"{self.api_url}/api/v1/batch-operations/{batch_operation_id}") as resp:
                    batch_status = await resp.json()
                    if batch_status['status'] == 'completed':
                        processing_complete = True
                poll_count += 1
            
            process_time = time.time() - process_start
        
        total_time = time.time() - start_time
        
        result = {
            "batch_size": batch_size,
            "upload_time": upload_time,
            "process_time": process_time,
            "total_time": total_time,
            "throughput": batch_size / total_time * 60,  # docs/min
            "avg_time_per_doc": total_time / batch_size,
            "cache_performance": batch_result.get('cache_performance', {})
        }
        
        self.results["batch_processing"].append(result)
        return result
    
    async def test_cache_performance(self, file_path: str, iterations: int = 3) -> Dict[str, Any]:
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
        results = []
        
        for i in range(iterations):
            result = await self.test_single_document(file_path)
            results.append(result['process_time'])
            
            # æ¸…ç†æ–‡æ¡£ä»¥ä¾¿é‡æ–°æµ‹è¯•
            await self._cleanup_document(result.get('document_id'))
        
        cache_result = {
            "file_path": file_path,
            "iterations": iterations,
            "first_process_time": results[0],
            "cached_process_times": results[1:],
            "cache_speedup": results[0] / statistics.mean(results[1:]) if len(results) > 1 else 1.0
        }
        
        self.results["cache_performance"].append(cache_result)
        return cache_result
    
    async def run_benchmark_suite(self, test_files: List[str]):
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•å¥—ä»¶"""
        print("ğŸš€ å¼€å§‹åŸºå‡†æ€§èƒ½æµ‹è¯•...")
        
        # 1. å•æ–‡æ¡£æµ‹è¯•
        print("\nğŸ“„ æµ‹è¯•å•æ–‡æ¡£å¤„ç†æ€§èƒ½...")
        for file_path in test_files[:5]:  # æµ‹è¯•å‰5ä¸ªæ–‡ä»¶
            result = await self.test_single_document(file_path)
            print(f"  âœ“ {Path(file_path).name}: {result['total_time']:.2f}s")
        
        # 2. æ‰¹å¤„ç†æµ‹è¯•
        print("\nğŸ“¦ æµ‹è¯•æ‰¹é‡å¤„ç†æ€§èƒ½...")
        for batch_size in [5, 10, 20]:
            result = await self.test_batch_processing(test_files, batch_size)
            print(f"  âœ“ Batch {batch_size}: {result['throughput']:.1f} docs/min")
        
        # 3. ç¼“å­˜æµ‹è¯•
        print("\nğŸ’¾ æµ‹è¯•ç¼“å­˜æ€§èƒ½...")
        cache_result = await self.test_cache_performance(test_files[0])
        print(f"  âœ“ Cache speedup: {cache_result['cache_speedup']:.2f}x")
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report()
    
    def _generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "single_document_stats": self._calculate_stats(self.results["single_document"], "total_time"),
            "batch_processing_stats": self._calculate_stats(self.results["batch_processing"], "throughput"),
            "cache_performance": self.results["cache_performance"],
            "detailed_results": self.results
        }
        
        with open("benchmark_report.json", "w") as f:
            json.dump(report, f, indent=2)
        
        print("\nğŸ“Š æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: benchmark_report.json")
        self._print_summary(report)
    
    def _calculate_stats(self, data: List[Dict], key: str) -> Dict[str, float]:
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        if not data:
            return {}
        
        values = [d[key] for d in data if key in d]
        return {
            "min": min(values),
            "max": max(values),
            "mean": statistics.mean(values),
            "median": statistics.median(values),
            "stdev": statistics.stdev(values) if len(values) > 1 else 0
        }
    
    def _print_summary(self, report: Dict):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*50)
        print("æ€§èƒ½æµ‹è¯•æ‘˜è¦")
        print("="*50)
        
        if "single_document_stats" in report:
            stats = report["single_document_stats"]
            print(f"\nå•æ–‡æ¡£å¤„ç†æ—¶é—´:")
            print(f"  å¹³å‡: {stats.get('mean', 0):.2f}s")
            print(f"  æœ€å¿«: {stats.get('min', 0):.2f}s")
            print(f"  æœ€æ…¢: {stats.get('max', 0):.2f}s")
        
        if "batch_processing_stats" in report:
            stats = report["batch_processing_stats"]
            print(f"\næ‰¹å¤„ç†ååé‡:")
            print(f"  å¹³å‡: {stats.get('mean', 0):.1f} docs/min")
            print(f"  æœ€é«˜: {stats.get('max', 0):.1f} docs/min")
            print(f"  æœ€ä½: {stats.get('min', 0):.1f} docs/min")
    
    async def _cleanup_document(self, document_id: str):
        """æ¸…ç†æµ‹è¯•æ–‡æ¡£"""
        if not document_id:
            return
        
        async with aiohttp.ClientSession() as session:
            await session.delete(f"{self.api_url}/api/v1/documents", 
                                json={"document_ids": [document_id]})


async def main():
    # å‡†å¤‡æµ‹è¯•æ–‡ä»¶
    test_files = list(Path("test_documents").glob("*.pdf"))[:20]
    
    if not test_files:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶ï¼Œè¯·åœ¨ test_documents ç›®å½•ä¸‹å‡†å¤‡æµ‹è¯•æ–‡æ¡£")
        return
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark = BenchmarkTest()
    await benchmark.run_benchmark_suite(test_files)


if __name__ == "__main__":
    asyncio.run(main())
```

### 2.2 è´Ÿè½½æµ‹è¯•è„šæœ¬ï¼ˆä½¿ç”¨Locustï¼‰

åˆ›å»ºæ–‡ä»¶ `performance_tests/locustfile.py`:

```python
from locust import HttpUser, task, between, events
import random
import time
from pathlib import Path
import json

class RAGAPIUser(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        """åˆå§‹åŒ–æµ‹è¯•æ•°æ®"""
        self.test_files = list(Path("test_documents").glob("*"))
        self.uploaded_documents = []
    
    @task(3)
    def upload_document(self):
        """ä¸Šä¼ æ–‡æ¡£ä»»åŠ¡"""
        if not self.test_files:
            return
        
        file_path = random.choice(self.test_files)
        
        with open(file_path, 'rb') as f:
            files = {'file': (file_path.name, f, 'application/octet-stream')}
            
            with self.client.post("/api/v1/documents/upload", 
                                 files=files, 
                                 catch_response=True) as response:
                if response.status_code == 200:
                    data = response.json()
                    self.uploaded_documents.append(data['document_id'])
                    response.success()
                else:
                    response.failure(f"Upload failed: {response.status_code}")
    
    @task(2)
    def process_document(self):
        """å¤„ç†æ–‡æ¡£ä»»åŠ¡"""
        if not self.uploaded_documents:
            self.upload_document()
            return
        
        document_id = random.choice(self.uploaded_documents)
        
        with self.client.post(f"/api/v1/documents/{document_id}/process",
                             catch_response=True) as response:
            if response.status_code == 200:
                response.success()
            else:
                response.failure(f"Process failed: {response.status_code}")
    
    @task(5)
    def query_documents(self):
        """æŸ¥è¯¢æ–‡æ¡£ä»»åŠ¡"""
        queries = [
            "What is the main topic?",
            "Summarize the key points",
            "Find information about technology",
            "What are the conclusions?",
            "Explain the methodology"
        ]
        
        query_data = {
            "query": random.choice(queries),
            "mode": random.choice(["hybrid", "local", "global"]),
            "vlm_enhanced": False
        }
        
        with self.client.post("/api/v1/query",
                             json=query_data,
                             catch_response=True) as response:
            if response.status_code == 200:
                response.success()
            else:
                response.failure(f"Query failed: {response.status_code}")
    
    @task(1)
    def batch_process(self):
        """æ‰¹é‡å¤„ç†ä»»åŠ¡"""
        if len(self.uploaded_documents) < 5:
            return
        
        batch_size = random.randint(3, 10)
        document_ids = random.sample(self.uploaded_documents, min(batch_size, len(self.uploaded_documents)))
        
        batch_data = {
            "document_ids": document_ids,
            "parser": "auto",
            "parse_method": "auto"
        }
        
        with self.client.post("/api/v1/documents/process/batch",
                             json=batch_data,
                             catch_response=True) as response:
            if response.status_code == 200:
                response.success()
            else:
                response.failure(f"Batch process failed: {response.status_code}")

# è‡ªå®šä¹‰ç»Ÿè®¡æ”¶é›†
@events.request.add_listener
def on_request(request_type, name, response_time, response_length, response, context, exception, **kwargs):
    """æ”¶é›†è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡"""
    if exception:
        print(f"Request failed: {name} - {exception}")

@events.test_stop.add_listener
def on_test_stop(environment, **kwargs):
    """æµ‹è¯•ç»“æŸæ—¶ç”ŸæˆæŠ¥å‘Š"""
    stats = environment.stats
    
    report = {
        "total_requests": stats.total.num_requests,
        "failure_rate": stats.total.failure_ratio,
        "avg_response_time": stats.total.avg_response_time,
        "median_response_time": stats.total.median_response_time,
        "p95_response_time": stats.total.get_response_time_percentile(0.95),
        "p99_response_time": stats.total.get_response_time_percentile(0.99),
        "rps": stats.total.current_rps
    }
    
    with open("load_test_report.json", "w") as f:
        json.dump(report, f, indent=2)
    
    print(f"\nè´Ÿè½½æµ‹è¯•å®Œæˆ:")
    print(f"  æ€»è¯·æ±‚æ•°: {report['total_requests']}")
    print(f"  å¤±è´¥ç‡: {report['failure_rate']:.2%}")
    print(f"  å¹³å‡å“åº”æ—¶é—´: {report['avg_response_time']:.2f}ms")
    print(f"  P95å“åº”æ—¶é—´: {report['p95_response_time']:.2f}ms")
    print(f"  RPS: {report['rps']:.2f}")
```

### 2.3 å¹¶å‘æµ‹è¯•è„šæœ¬

åˆ›å»ºæ–‡ä»¶ `performance_tests/concurrency_test.py`:

```python
#!/usr/bin/env python3
"""
å¹¶å‘å¤„ç†èƒ½åŠ›æµ‹è¯•
æµ‹è¯•ç³»ç»Ÿåœ¨ä¸åŒå¹¶å‘çº§åˆ«ä¸‹çš„è¡¨ç°
"""

import asyncio
import aiohttp
import time
from typing import List, Dict, Any
import statistics
import json

class ConcurrencyTest:
    def __init__(self, api_url: str = "http://127.0.0.1:8001"):
        self.api_url = api_url
        self.results = []
    
    async def concurrent_upload_and_process(self, file_paths: List[str], concurrency: int):
        """å¹¶å‘ä¸Šä¼ å’Œå¤„ç†æ–‡æ¡£"""
        semaphore = asyncio.Semaphore(concurrency)
        
        async def process_single_file(file_path: str) -> Dict[str, Any]:
            async with semaphore:
                start_time = time.time()
                
                async with aiohttp.ClientSession() as session:
                    # ä¸Šä¼ 
                    with open(file_path, 'rb') as f:
                        data = aiohttp.FormData()
                        data.add_field('file', f, filename=Path(file_path).name)
                        
                        async with session.post(f"{self.api_url}/api/v1/documents/upload", data=data) as resp:
                            upload_result = await resp.json()
                    
                    document_id = upload_result['document_id']
                    
                    # å¤„ç†
                    async with session.post(f"{self.api_url}/api/v1/documents/{document_id}/process") as resp:
                        process_result = await resp.json()
                    
                    task_id = process_result['task_id']
                    
                    # è½®è¯¢çŠ¶æ€
                    completed = False
                    polls = 0
                    while not completed and polls < 300:
                        await asyncio.sleep(1)
                        async with session.get(f"{self.api_url}/api/v1/tasks/{task_id}") as resp:
                            task_status = await resp.json()
                            if task_status['task']['status'] in ['completed', 'failed']:
                                completed = True
                        polls += 1
                
                total_time = time.time() - start_time
                
                return {
                    "file_path": file_path,
                    "time": total_time,
                    "success": task_status['task']['status'] == 'completed'
                }
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [process_single_file(fp) for fp in file_paths]
        
        # æ‰§è¡Œå¹¶æ”¶é›†ç»“æœ
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time
        
        # åˆ†æç»“æœ
        successful = [r for r in results if isinstance(r, dict) and r.get('success')]
        failed = len(results) - len(successful)
        times = [r['time'] for r in successful]
        
        return {
            "concurrency": concurrency,
            "total_files": len(file_paths),
            "successful": len(successful),
            "failed": failed,
            "total_time": total_time,
            "throughput": len(successful) / total_time * 60,  # docs/min
            "avg_time": statistics.mean(times) if times else 0,
            "median_time": statistics.median(times) if times else 0,
            "min_time": min(times) if times else 0,
            "max_time": max(times) if times else 0
        }
    
    async def run_concurrency_tests(self, file_paths: List[str]):
        """è¿è¡Œä¸åŒå¹¶å‘çº§åˆ«çš„æµ‹è¯•"""
        concurrency_levels = [1, 2, 5, 10, 20, 50]
        
        for level in concurrency_levels:
            print(f"\næµ‹è¯•å¹¶å‘çº§åˆ«: {level}")
            
            # ä½¿ç”¨éƒ¨åˆ†æ–‡ä»¶è¿›è¡Œæµ‹è¯•
            test_files = file_paths[:min(level * 2, len(file_paths))]
            
            result = await self.concurrent_upload_and_process(test_files, level)
            self.results.append(result)
            
            print(f"  âœ“ ååé‡: {result['throughput']:.1f} docs/min")
            print(f"  âœ“ å¹³å‡æ—¶é—´: {result['avg_time']:.2f}s")
            print(f"  âœ“ æˆåŠŸç‡: {result['successful']}/{result['total_files']}")
        
        self._generate_report()
    
    def _generate_report(self):
        """ç”Ÿæˆå¹¶å‘æµ‹è¯•æŠ¥å‘Š"""
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "results": self.results,
            "optimal_concurrency": self._find_optimal_concurrency()
        }
        
        with open("concurrency_test_report.json", "w") as f:
            json.dump(report, f, indent=2)
        
        print("\nğŸ“Š å¹¶å‘æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: concurrency_test_report.json")
        self._plot_results()
    
    def _find_optimal_concurrency(self) -> int:
        """æ‰¾å‡ºæœ€ä¼˜å¹¶å‘çº§åˆ«"""
        if not self.results:
            return 1
        
        # åŸºäºååé‡æ‰¾å‡ºæœ€ä¼˜å¹¶å‘
        best = max(self.results, key=lambda x: x['throughput'])
        return best['concurrency']
    
    def _plot_results(self):
        """ç»˜åˆ¶ç»“æœå›¾è¡¨ï¼ˆå¦‚æœmatplotlibå¯ç”¨ï¼‰"""
        try:
            import matplotlib.pyplot as plt
            
            concurrency = [r['concurrency'] for r in self.results]
            throughput = [r['throughput'] for r in self.results]
            avg_time = [r['avg_time'] for r in self.results]
            
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
            
            # ååé‡å›¾
            ax1.plot(concurrency, throughput, 'b-o')
            ax1.set_xlabel('å¹¶å‘çº§åˆ«')
            ax1.set_ylabel('ååé‡ (docs/min)')
            ax1.set_title('å¹¶å‘çº§åˆ« vs ååé‡')
            ax1.grid(True)
            
            # å“åº”æ—¶é—´å›¾
            ax2.plot(concurrency, avg_time, 'r-o')
            ax2.set_xlabel('å¹¶å‘çº§åˆ«')
            ax2.set_ylabel('å¹³å‡å“åº”æ—¶é—´ (s)')
            ax2.set_title('å¹¶å‘çº§åˆ« vs å“åº”æ—¶é—´')
            ax2.grid(True)
            
            plt.tight_layout()
            plt.savefig('concurrency_test_results.png')
            print("  âœ“ å›¾è¡¨å·²ä¿å­˜: concurrency_test_results.png")
        except ImportError:
            pass
```

## 3. æ‰§è¡Œæµ‹è¯•è®¡åˆ’

### 3.1 æµ‹è¯•æ‰§è¡Œæ­¥éª¤

```bash
# 1. å‡†å¤‡æµ‹è¯•ç¯å¢ƒ
mkdir -p performance_tests test_documents
cd performance_tests

# 2. å‡†å¤‡æµ‹è¯•æ•°æ®
python generate_test_documents.py

# 3. å¯åŠ¨ç›‘æ§
python system_monitor.py &

# 4. æ‰§è¡ŒåŸºå‡†æµ‹è¯•
python benchmark_test.py

# 5. æ‰§è¡Œè´Ÿè½½æµ‹è¯•
locust -f locustfile.py --host=http://127.0.0.1:8001 \
       --users=50 --spawn-rate=2 --time=10m \
       --html=load_test_report.html

# 6. æ‰§è¡Œå¹¶å‘æµ‹è¯•
python concurrency_test.py

# 7. æ‰§è¡Œå‹åŠ›æµ‹è¯•
locust -f locustfile.py --host=http://127.0.0.1:8001 \
       --users=200 --spawn-rate=10 --time=5m \
       --html=stress_test_report.html

# 8. æ‰§è¡ŒæŒä¹…æ€§æµ‹è¯•
python endurance_test.py --duration=3600  # 1å°æ—¶
```

### 3.2 ç›‘æ§è„šæœ¬

åˆ›å»ºæ–‡ä»¶ `performance_tests/system_monitor.py`:

```python
#!/usr/bin/env python3
"""
ç³»ç»Ÿèµ„æºç›‘æ§è„šæœ¬
å®æ—¶ç›‘æ§CPUã€å†…å­˜ã€GPUå’Œç£ç›˜ä½¿ç”¨æƒ…å†µ
"""

import psutil
import GPUtil
import time
import json
from datetime import datetime
import threading

class SystemMonitor:
    def __init__(self, interval: int = 5):
        self.interval = interval
        self.monitoring = False
        self.data = []
    
    def start(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        thread = threading.Thread(target=self._monitor_loop)
        thread.daemon = True
        thread.start()
        print(f"ğŸ“Š ç³»ç»Ÿç›‘æ§å·²å¯åŠ¨ (é—´éš”: {self.interval}ç§’)")
    
    def stop(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        self._save_data()
        print("ğŸ“Š ç³»ç»Ÿç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            metrics = self._collect_metrics()
            self.data.append(metrics)
            
            # å®æ—¶æ˜¾ç¤ºå…³é”®æŒ‡æ ‡
            print(f"\r[{metrics['timestamp']}] "
                  f"CPU: {metrics['cpu_percent']:.1f}% | "
                  f"MEM: {metrics['memory_percent']:.1f}% | "
                  f"DISK I/O: R={metrics['disk_read_mb']:.1f}MB/s W={metrics['disk_write_mb']:.1f}MB/s",
                  end="")
            
            time.sleep(self.interval)
    
    def _collect_metrics(self) -> dict:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        # CPUæŒ‡æ ‡
        cpu_percent = psutil.cpu_percent(interval=1)
        cpu_per_core = psutil.cpu_percent(interval=1, percpu=True)
        
        # å†…å­˜æŒ‡æ ‡
        memory = psutil.virtual_memory()
        swap = psutil.swap_memory()
        
        # ç£ç›˜I/O
        disk_io = psutil.disk_io_counters()
        disk_usage = psutil.disk_usage('/')
        
        # ç½‘ç»œI/O
        net_io = psutil.net_io_counters()
        
        # GPUæŒ‡æ ‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        gpu_metrics = {}
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                gpu_metrics = {
                    "gpu_load": gpu.load * 100,
                    "gpu_memory_used": gpu.memoryUsed,
                    "gpu_memory_total": gpu.memoryTotal,
                    "gpu_temperature": gpu.temperature
                }
        except:
            pass
        
        return {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": cpu_percent,
            "cpu_per_core": cpu_per_core,
            "memory_percent": memory.percent,
            "memory_available_gb": memory.available / (1024**3),
            "memory_used_gb": memory.used / (1024**3),
            "swap_percent": swap.percent,
            "disk_usage_percent": disk_usage.percent,
            "disk_read_mb": disk_io.read_bytes / (1024**2) / self.interval,
            "disk_write_mb": disk_io.write_bytes / (1024**2) / self.interval,
            "network_sent_mb": net_io.bytes_sent / (1024**2) / self.interval,
            "network_recv_mb": net_io.bytes_recv / (1024**2) / self.interval,
            **gpu_metrics
        }
    
    def _save_data(self):
        """ä¿å­˜ç›‘æ§æ•°æ®"""
        with open(f"system_metrics_{int(time.time())}.json", "w") as f:
            json.dump(self.data, f, indent=2)
        
        # ç”Ÿæˆæ‘˜è¦
        self._generate_summary()
    
    def _generate_summary(self):
        """ç”Ÿæˆç›‘æ§æ‘˜è¦"""
        if not self.data:
            return
        
        summary = {
            "duration_seconds": len(self.data) * self.interval,
            "cpu_avg": sum(d['cpu_percent'] for d in self.data) / len(self.data),
            "cpu_max": max(d['cpu_percent'] for d in self.data),
            "memory_avg": sum(d['memory_percent'] for d in self.data) / len(self.data),
            "memory_max": max(d['memory_percent'] for d in self.data)
        }
        
        with open("monitoring_summary.json", "w") as f:
            json.dump(summary, f, indent=2)
        
        print(f"\nğŸ“Š ç›‘æ§æ‘˜è¦:")
        print(f"  å¹³å‡CPU: {summary['cpu_avg']:.1f}%")
        print(f"  å³°å€¼CPU: {summary['cpu_max']:.1f}%")
        print(f"  å¹³å‡å†…å­˜: {summary['memory_avg']:.1f}%")
        print(f"  å³°å€¼å†…å­˜: {summary['memory_max']:.1f}%")

if __name__ == "__main__":
    monitor = SystemMonitor(interval=5)
    monitor.start()
    
    try:
        # ä¿æŒè¿è¡Œ
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        monitor.stop()
```

## 4. æ€§èƒ½æŒ‡æ ‡æ”¶é›†

### 4.1 å…³é”®æ€§èƒ½æŒ‡æ ‡ (KPIs)

| æŒ‡æ ‡ç±»åˆ« | å…·ä½“æŒ‡æ ‡ | ç›®æ ‡å€¼ | å‘Šè­¦é˜ˆå€¼ |
|---------|---------|--------|---------|
| **å“åº”æ—¶é—´** | | | |
| P50 å“åº”æ—¶é—´ | ä¸­ä½æ•°å“åº”æ—¶é—´ | < 2s | > 5s |
| P95 å“åº”æ—¶é—´ | 95åˆ†ä½å“åº”æ—¶é—´ | < 10s | > 30s |
| P99 å“åº”æ—¶é—´ | 99åˆ†ä½å“åº”æ—¶é—´ | < 30s | > 60s |
| **ååé‡** | | | |
| æ–‡æ¡£å¤„ç†é€Ÿç‡ | docs/min | > 60 | < 20 |
| å¹¶å‘å¤„ç†èƒ½åŠ› | å¹¶å‘æ•° | > 20 | < 5 |
| æ‰¹å¤„ç†æ•ˆç‡ | ç›¸å¯¹ä¸²è¡Œæå‡ | > 5x | < 2x |
| **èµ„æºä½¿ç”¨** | | | |
| CPUåˆ©ç”¨ç‡ | ç™¾åˆ†æ¯” | 60-80% | > 90% |
| å†…å­˜ä½¿ç”¨ç‡ | ç™¾åˆ†æ¯” | < 70% | > 85% |
| GPUåˆ©ç”¨ç‡ | ç™¾åˆ†æ¯” | 70-90% | < 30% |
| **å¯é æ€§** | | | |
| æˆåŠŸç‡ | ç™¾åˆ†æ¯” | > 99% | < 95% |
| é”™è¯¯ç‡ | ç™¾åˆ†æ¯” | < 1% | > 5% |
| ç¼“å­˜å‘½ä¸­ç‡ | ç™¾åˆ†æ¯” | > 60% | < 30% |

### 4.2 æµ‹è¯•ç»“æœåˆ†ææ¨¡æ¿

```python
# performance_tests/analyze_results.py

def analyze_test_results():
    """åˆ†ææ‰€æœ‰æµ‹è¯•ç»“æœå¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    
    # åŠ è½½å„é¡¹æµ‹è¯•ç»“æœ
    benchmark = json.load(open("benchmark_report.json"))
    load_test = json.load(open("load_test_report.json"))
    concurrency = json.load(open("concurrency_test_report.json"))
    monitoring = json.load(open("monitoring_summary.json"))
    
    # ç»¼åˆåˆ†æ
    analysis = {
        "performance_score": calculate_performance_score(benchmark, load_test),
        "bottlenecks": identify_bottlenecks(monitoring),
        "optimization_recommendations": generate_recommendations(all_results),
        "comparison_with_baseline": compare_with_baseline(current, baseline)
    }
    
    # ç”ŸæˆHTMLæŠ¥å‘Š
    generate_html_report(analysis)
```

## 5. æŒç»­æ€§èƒ½ç›‘æ§

### 5.1 Prometheus + Grafana é…ç½®

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'rag-api'
    static_configs:
      - targets: ['localhost:8001']
    metrics_path: '/metrics'
```

### 5.2 è‡ªå®šä¹‰æŒ‡æ ‡å¯¼å‡º

```python
# performance_tests/metrics_exporter.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

# å®šä¹‰æŒ‡æ ‡
document_processed = Counter('documents_processed_total', 'Total processed documents')
processing_time = Histogram('document_processing_seconds', 'Document processing time')
active_tasks = Gauge('active_tasks', 'Number of active tasks')
cache_hit_rate = Gauge('cache_hit_rate', 'Cache hit rate percentage')

def export_metrics():
    """å¯¼å‡ºè‡ªå®šä¹‰æŒ‡æ ‡åˆ°Prometheus"""
    start_http_server(9090)
    
    while True:
        # æ›´æ–°æŒ‡æ ‡
        update_metrics_from_api()
        time.sleep(10)
```

## 6. æµ‹è¯•æŠ¥å‘Šæ¨¡æ¿

### 6.1 æ‰§è¡Œæ‘˜è¦
- æµ‹è¯•æ—¥æœŸå’Œç¯å¢ƒ
- ä¸»è¦å‘ç°
- æ€§èƒ½è¯„åˆ†
- å…³é”®å»ºè®®

### 6.2 è¯¦ç»†ç»“æœ
- å„é¡¹æµ‹è¯•æ•°æ®
- å›¾è¡¨å’Œè¶‹åŠ¿
- ä¸åŸºçº¿å¯¹æ¯”
- å¼‚å¸¸å’Œé—®é¢˜

### 6.3 ä¼˜åŒ–å»ºè®®
- ç«‹å³è¡ŒåŠ¨é¡¹
- çŸ­æœŸæ”¹è¿›
- é•¿æœŸè§„åˆ’

### 6.4 é™„å½•
- åŸå§‹æ•°æ®
- æµ‹è¯•è„šæœ¬
- ç³»ç»Ÿé…ç½®

## ç»“è®º

æœ¬æµ‹è¯•æ–¹æ¡ˆæä¾›äº†å…¨é¢çš„æ€§èƒ½æµ‹è¯•æ¡†æ¶ï¼ŒåŒ…æ‹¬ï¼š
- **5ç§æµ‹è¯•ç±»å‹**è¦†ç›–ä¸åŒæ€§èƒ½æ–¹é¢
- **è‡ªåŠ¨åŒ–æµ‹è¯•è„šæœ¬**å‡å°‘äººå·¥æ“ä½œ
- **å®æ—¶ç›‘æ§**æ•è·ç³»ç»Ÿè¡Œä¸º
- **è¯¦ç»†æŠ¥å‘Š**æ”¯æŒå†³ç­–

å»ºè®®æ¯æ¬¡é‡å¤§æ›´æ–°åæ‰§è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶ï¼Œæ—¥å¸¸è¿›è¡ŒåŸºå‡†æµ‹è¯•ç›‘æ§æ€§èƒ½é€€åŒ–ã€‚
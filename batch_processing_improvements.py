#!/usr/bin/env python3
"""
æ‰¹é‡å¤„ç†æ”¹è¿›æ–¹æ¡ˆ
å¢å¼ºæ‰¹é‡å¤„ç†çš„é”™è¯¯æ£€æµ‹å’Œæ•°æ®ä¸€è‡´æ€§éªŒè¯
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime

logger = logging.getLogger(__name__)

class EnhancedBatchProcessor:
    """å¢å¼ºçš„æ‰¹é‡å¤„ç†å™¨ï¼ŒåŒ…å«æ•°æ®ä¸€è‡´æ€§éªŒè¯"""
    
    def __init__(self, rag_storage_dir: str):
        self.rag_storage_dir = Path(rag_storage_dir)
        
    async def verify_document_consistency(self, doc_id: str, file_path: str) -> Dict:
        """éªŒè¯å•ä¸ªæ–‡æ¡£çš„æ•°æ®ä¸€è‡´æ€§"""
        verification_result = {
            "doc_id": doc_id,
            "file_path": file_path,
            "is_consistent": True,
            "missing_components": [],
            "errors": [],
            "verification_timestamp": datetime.now().isoformat()
        }
        
        try:
            # æ£€æŸ¥doc_status
            doc_status_file = self.rag_storage_dir / "kv_store_doc_status.json"
            if doc_status_file.exists():
                with open(doc_status_file, 'r', encoding='utf-8') as f:
                    doc_status = json.load(f)
                
                if doc_id not in doc_status:
                    verification_result["missing_components"].append("doc_status")
                    verification_result["is_consistent"] = False
                elif doc_status[doc_id].get("status") != "processed":
                    verification_result["errors"].append(
                        f"Document status is '{doc_status[doc_id].get('status')}', expected 'processed'"
                    )
                    verification_result["is_consistent"] = False
            
            # æ£€æŸ¥full_docs
            full_docs_file = self.rag_storage_dir / "kv_store_full_docs.json"
            if full_docs_file.exists():
                with open(full_docs_file, 'r', encoding='utf-8') as f:
                    full_docs = json.load(f)
                
                if doc_id not in full_docs:
                    verification_result["missing_components"].append("full_docs")
                    verification_result["is_consistent"] = False
            
            # æ£€æŸ¥chunks
            chunks_file = self.rag_storage_dir / "vdb_chunks.json"
            if chunks_file.exists():
                with open(chunks_file, 'r', encoding='utf-8') as f:
                    chunks_data = json.load(f)
                
                doc_chunks = [c for c in chunks_data.get("data", []) if c.get("full_doc_id") == doc_id]
                if not doc_chunks:
                    verification_result["missing_components"].append("chunks")
                    verification_result["is_consistent"] = False
                    
        except Exception as e:
            verification_result["errors"].append(f"Verification error: {str(e)}")
            verification_result["is_consistent"] = False
            
        return verification_result
    
    async def enhanced_batch_process_with_verification(
        self, 
        file_paths: List[str],
        rag_instance,
        progress_callback=None,
        **kwargs
    ) -> Dict:
        """å¢å¼ºçš„æ‰¹é‡å¤„ç†ï¼ŒåŒ…å«ä¸€è‡´æ€§éªŒè¯"""
        
        logger.info(f"å¼€å§‹å¢å¼ºæ‰¹é‡å¤„ç†: {len(file_paths)} ä¸ªæ–‡ä»¶")
        
        results = {
            "total_files": len(file_paths),
            "successful": 0,
            "failed": 0,
            "verified_consistent": 0,
            "verified_inconsistent": 0,
            "processing_results": [],
            "verification_results": [],
            "inconsistent_documents": [],
            "processing_timestamp": datetime.now().isoformat()
        }
        
        # ç¬¬ä¸€é˜¶æ®µï¼šå¸¸è§„æ‰¹é‡å¤„ç†
        batch_result = await rag_instance.process_folder_complete(
            folder_path=str(Path(file_paths[0]).parent),
            **kwargs
        )
        
        # ç¬¬äºŒé˜¶æ®µï¼šéªŒè¯æ¯ä¸ªæ–‡æ¡£çš„ä¸€è‡´æ€§
        logger.info("å¼€å§‹æ•°æ®ä¸€è‡´æ€§éªŒè¯é˜¶æ®µ...")
        
        for i, file_path in enumerate(file_paths):
            try:
                # ç”Ÿæˆæ–‡æ¡£ID (æ¨¡æ‹ŸRAGç³»ç»Ÿçš„IDç”Ÿæˆé€»è¾‘)
                import hashlib
                doc_id = f"doc-{hashlib.md5(file_path.encode()).hexdigest()}"
                
                # éªŒè¯æ–‡æ¡£ä¸€è‡´æ€§
                verification = await self.verify_document_consistency(doc_id, file_path)
                results["verification_results"].append(verification)
                
                if verification["is_consistent"]:
                    results["verified_consistent"] += 1
                    results["successful"] += 1
                else:
                    results["verified_inconsistent"] += 1
                    results["failed"] += 1
                    results["inconsistent_documents"].append({
                        "file_path": file_path,
                        "doc_id": doc_id,
                        "missing_components": verification["missing_components"],
                        "errors": verification["errors"]
                    })
                    
                    logger.warning(f"æ•°æ®ä¸ä¸€è‡´æ£€æµ‹: {Path(file_path).name}")
                    for error in verification["errors"]:
                        logger.warning(f"  - {error}")
                
                # è¿›åº¦å›è°ƒ
                if progress_callback:
                    await progress_callback({
                        "stage": "verification",
                        "current": i + 1,
                        "total": len(file_paths),
                        "progress": (i + 1) / len(file_paths) * 100,
                        "current_file": Path(file_path).name,
                        "is_consistent": verification["is_consistent"]
                    })
                    
            except Exception as e:
                logger.error(f"éªŒè¯æ–‡æ¡£å¤±è´¥ {file_path}: {str(e)}")
                results["failed"] += 1
                results["inconsistent_documents"].append({
                    "file_path": file_path,
                    "doc_id": "unknown",
                    "errors": [f"Verification failed: {str(e)}"]
                })
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {results['successful']} æˆåŠŸ, {results['failed']} å¤±è´¥")
        logger.info(f"ä¸€è‡´æ€§éªŒè¯: {results['verified_consistent']} ä¸€è‡´, {results['verified_inconsistent']} ä¸ä¸€è‡´")
        
        return results

def create_improved_batch_endpoint_patch():
    """åˆ›å»ºæ”¹è¿›çš„æ‰¹é‡å¤„ç†ç«¯ç‚¹è¡¥ä¸"""
    
    patch_code = '''
# åœ¨ /api/v1/documents/process/batch ç«¯ç‚¹ä¸­çš„æ”¹è¿›

async def process_documents_batch_enhanced(request: BatchProcessRequest):
    """å¢å¼ºçš„æ‰¹é‡æ–‡æ¡£å¤„ç†ç«¯ç‚¹ - åŒ…å«æ•°æ®ä¸€è‡´æ€§éªŒè¯"""
    
    # ... ç°æœ‰çš„åˆå§‹åŒ–ä»£ç  ...
    
    try:
        # ä½¿ç”¨å¢å¼ºçš„æ‰¹é‡å¤„ç†å™¨
        enhanced_processor = EnhancedBatchProcessor(WORKING_DIR)
        
        # æ‰§è¡Œå¢å¼ºçš„æ‰¹é‡å¤„ç†
        enhanced_result = await enhanced_processor.enhanced_batch_process_with_verification(
            file_paths=file_paths,
            rag_instance=rag,
            progress_callback=websocket_progress_callback,
            output_dir=OUTPUT_DIR,
            parse_method=parse_method,
            max_workers=max_workers,
            recursive=False,
            show_progress=True,
            lang="en",
            device=device_type
        )
        
        # å¤„ç†ç»“æœï¼Œé‡ç‚¹å…³æ³¨ä¸ä¸€è‡´çš„æ–‡æ¡£
        for file_path in file_paths:
            doc_info = path_to_doc[file_path]
            document_id = doc_info["document_id"]
            document = doc_info["document"]
            task_id = doc_info["task_id"]
            
            # æ£€æŸ¥è¯¥æ–‡ä»¶çš„ä¸€è‡´æ€§éªŒè¯ç»“æœ
            file_verification = next(
                (v for v in enhanced_result["verification_results"] if v["file_path"] == file_path),
                None
            )
            
            if file_verification and file_verification["is_consistent"]:
                # ä¸€è‡´æ€§éªŒè¯é€šè¿‡
                document["status"] = "completed"
                tasks[task_id]["status"] = "completed"
                tasks[task_id]["completed_at"] = datetime.now().isoformat()
                
                results.append({
                    "document_id": document_id,
                    "file_name": document["file_name"],
                    "status": "success",
                    "message": "æ–‡æ¡£æ‰¹é‡å¤„ç†æˆåŠŸå¹¶é€šè¿‡ä¸€è‡´æ€§éªŒè¯",
                    "task_id": task_id,
                    "verification_passed": True
                })
                started_count += 1
            else:
                # ä¸€è‡´æ€§éªŒè¯å¤±è´¥
                error_details = []
                if file_verification:
                    error_details = file_verification.get("errors", [])
                    missing_components = file_verification.get("missing_components", [])
                    if missing_components:
                        error_details.append(f"ç¼ºå¤±ç»„ä»¶: {', '.join(missing_components)}")
                
                error_msg = f"æ•°æ®ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {'; '.join(error_details)}"
                
                document["status"] = "failed"
                document["error_category"] = "data_inconsistency"
                tasks[task_id]["status"] = "failed"
                tasks[task_id]["error"] = error_msg
                tasks[task_id]["updated_at"] = datetime.now().isoformat()
                
                results.append({
                    "document_id": document_id,
                    "file_name": document["file_name"],
                    "status": "failed",
                    "message": error_msg,
                    "task_id": task_id,
                    "verification_passed": False,
                    "missing_components": file_verification.get("missing_components", []) if file_verification else []
                })
                failed_count += 1
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        response_data = {
            "success": failed_count == 0,
            "started_count": started_count,
            "failed_count": failed_count,
            "total_requested": len(request.document_ids),
            "results": results,
            "batch_operation_id": batch_operation_id,
            "message": f"å¢å¼ºæ‰¹é‡å¤„ç†å®Œæˆ: {started_count} ä¸ªæˆåŠŸ, {failed_count} ä¸ªå¤±è´¥",
            "verification_summary": {
                "verified_consistent": enhanced_result["verified_consistent"],
                "verified_inconsistent": enhanced_result["verified_inconsistent"],
                "inconsistent_documents": len(enhanced_result["inconsistent_documents"])
            }
        }
        
        # å¦‚æœæœ‰ä¸ä¸€è‡´çš„æ–‡æ¡£ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
        if enhanced_result["inconsistent_documents"]:
            await send_processing_log(
                f"âš ï¸  å‘ç° {len(enhanced_result['inconsistent_documents'])} ä¸ªæ•°æ®ä¸ä¸€è‡´çš„æ–‡æ¡£", 
                "warning"
            )
            
            # è‡ªåŠ¨ç”Ÿæˆä¿®å¤å»ºè®®
            await send_processing_log(
                f"ğŸ’¡ å»ºè®®è¿è¡Œæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å™¨è¿›è¡Œä¿®å¤: python data_consistency_checker.py", 
                "info"
            )
        
        return BatchProcessResponse(**response_data)
        
    except Exception as e:
        # ç°æœ‰çš„é”™è¯¯å¤„ç†é€»è¾‘...
        pass
    '''
    
    return patch_code

if __name__ == "__main__":
    # ç”Ÿæˆæ”¹è¿›æ–¹æ¡ˆçš„ç¤ºä¾‹ä»£ç 
    patch_code = create_improved_batch_endpoint_patch()
    
    with open("batch_processing_endpoint_improvements.py", "w", encoding="utf-8") as f:
        f.write(patch_code)
    
    print("âœ… æ‰¹é‡å¤„ç†æ”¹è¿›æ–¹æ¡ˆå·²ç”Ÿæˆ")
    print("ğŸ“ æ–‡ä»¶: batch_processing_endpoint_improvements.py")
    print("ğŸ”§ å¯ä»¥å‚è€ƒæ­¤ä»£ç æ¥æ”¹è¿›APIç«¯ç‚¹çš„æ‰¹é‡å¤„ç†é€»è¾‘")
#!/usr/bin/env python3
"""
RAG-Anything Web API Server

FastAPIæœåŠ¡å™¨ï¼Œæä¾›Web UIä¸RAG-Anythingç³»ç»Ÿçš„æ¥å£
"""

import os
import sys
import asyncio
import logging
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
# ä¼˜å…ˆåŠ è½½RAG-Anythingç›®å½•ä¸‹çš„çœŸå®é…ç½®
load_dotenv("/home/ragsvr/projects/ragsystem/RAG-Anything/.env")
load_dotenv()  # åå¤‡åŠ è½½å½“å‰ç›®å½•çš„.env
from typing import List, Dict, Any, Optional
import tempfile
import json
from datetime import datetime

from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ¥å…¥å®é™…RAG-Anythingç”Ÿäº§ç³»ç»Ÿ
project_root = Path(__file__).parent.parent / "RAG-Anything"
sys.path.insert(0, str(project_root))

try:
    from raganything import RAGAnything, RAGAnythingConfig
    from lightrag.llm.openai import openai_complete_if_cache, openai_embed
    from lightrag.utils import EmbeddingFunc
    RAG_AVAILABLE = True
    logger.info("âœ… RAG-Anythingæ ¸å¿ƒç³»ç»Ÿå·²å¯¼å…¥")
except ImportError as e:
    RAG_AVAILABLE = False
    logger.warning(f"âš ï¸ RAG-Anythingæ ¸å¿ƒç³»ç»Ÿå¯¼å…¥å¤±è´¥: {e}")
    logger.warning("ğŸ”„ å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼è¿è¡Œ")

# FastAPIåº”ç”¨
app = FastAPI(
    title="RAG-Anything Web API",
    description="å¤šæ¨¡æ€RAGç³»ç»ŸWebæ¥å£",
    version="1.0.0"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],  # Reactå¼€å‘æœåŠ¡å™¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ·»åŠ é™æ€æ–‡ä»¶æœåŠ¡
app.mount("/webui", StaticFiles(directory="/home/ragsvr/projects/ragsystem/RAG-Anything/webui", html=True), name="webui")

# å…¨å±€RAGå®ä¾‹
rag_instance: Optional[RAGAnything] = None

# Pydanticæ¨¡å‹
class QueryRequest(BaseModel):
    query: str
    mode: str = "hybrid"
    multimodal_content: Optional[List[Dict[str, Any]]] = None

class QueryResponse(BaseModel):
    result: str
    query_id: str
    timestamp: datetime
    mode: str
    processing_time: float

class ConfigUpdate(BaseModel):
    section: str
    settings: Dict[str, Any]

class SystemStatus(BaseModel):
    status: str
    services: Dict[str, Any]
    metrics: Dict[str, Any]
    processing_stats: Dict[str, Any]

class LightragStatus(BaseModel):
    status: str = "healthy"
    working_directory: str
    input_directory: str = ""
    configuration: Dict[str, Any]
    pipeline_busy: bool = False
    core_version: Optional[str] = "1.0.0"
    api_version: Optional[str] = "1.0.0"
    auth_mode: Optional[str] = "disabled"
    webui_title: Optional[str] = "RAG-Anything WebUI"
    webui_description: Optional[str] = "å¤šæ¨¡æ€RAGç³»ç»ŸWebç•Œé¢"

class FileUploadResponse(BaseModel):
    file_id: str
    filename: str
    size: int
    status: str
    message: str

# ä¾èµ–å‡½æ•°
async def get_rag_instance() -> RAGAnything:
    """è·å–RAGå®ä¾‹ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆå§‹åŒ–"""
    global rag_instance
    
    if not RAG_AVAILABLE:
        # å¦‚æœRAGæ ¸å¿ƒç³»ç»Ÿä¸å¯ç”¨ï¼Œè¿”å›æ¨¡æ‹Ÿå¯¹è±¡
        return {"status": "demo_mode", "message": "RAGç³»ç»Ÿæ¼”ç¤ºæ¨¡å¼"}
    
    if rag_instance is None:
        try:
            # åˆå§‹åŒ–RAGé…ç½®ï¼ŒæŒ‡å‘æ­£ç¡®çš„å·¥ä½œç›®å½•
            config = RAGAnythingConfig(
                working_dir="/home/ragsvr/projects/ragsystem/RAG-Anything/rag_storage",
                parser="mineru",
                parse_method="auto",
                enable_image_processing=True,
                enable_table_processing=True,
                enable_equation_processing=True,
            )
            
            # æ£€æŸ¥DeepSeek APIé…ç½®
            deepseek_api_key = os.getenv("DEEPSEEK_API_KEY") or os.getenv("LLM_BINDING_API_KEY")
            deepseek_base_url = os.getenv("LLM_BINDING_HOST", "https://api.deepseek.com/v1")
            
            if deepseek_api_key:
                logger.info("ğŸš€ ä½¿ç”¨DeepSeek APIä½œä¸ºLLMåç«¯")
                # ä½¿ç”¨DeepSeek API
                from lightrag.llm.openai import openai_complete_if_cache, openai_embed
                
                # é…ç½®DeepSeek API
                os.environ["OPENAI_API_KEY"] = deepseek_api_key
                os.environ["OPENAI_BASE_URL"] = deepseek_base_url
                
                # æŒ‰ç…§native_with_qwen.pyçš„æˆåŠŸæ¨¡å¼å®šä¹‰LLMå‡½æ•°
                def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
                    return openai_complete_if_cache(
                        "deepseek-chat",
                        prompt,
                        system_prompt=system_prompt,
                        history_messages=history_messages,
                        api_key=deepseek_api_key,
                        base_url=deepseek_base_url,
                        **kwargs,
                    )

                # æ·»åŠ vision_model_funcï¼Œä¸native_with_qwen.pyä¿æŒä¸€è‡´
                def vision_model_func(
                    prompt,
                    system_prompt=None,
                    history_messages=[],
                    image_data=None,
                    messages=None,
                    **kwargs,
                ):
                    # If messages format is provided (for multimodal VLM enhanced query), use it directly
                    if messages:
                        return openai_complete_if_cache(
                            "deepseek-vl",
                            "",
                            system_prompt=None,
                            history_messages=[],
                            messages=messages,
                            api_key=deepseek_api_key,
                            base_url=deepseek_base_url,
                            **kwargs,
                        )
                    # Traditional single image format
                    elif image_data:
                        return openai_complete_if_cache(
                            "deepseek-vl",
                            "",
                            system_prompt=None,
                            history_messages=[],
                            messages=[
                                {"role": "system", "content": system_prompt}
                                if system_prompt
                                else None,
                                {
                                    "role": "user",
                                    "content": [
                                        {"type": "text", "text": prompt},
                                        {
                                            "type": "image_url",
                                            "image_url": {
                                                "url": f"data:image/jpeg;base64,{image_data}"
                                            },
                                        },
                                    ],
                                }
                                if image_data
                                else {"role": "user", "content": prompt},
                            ],
                            api_key=deepseek_api_key,
                            base_url=deepseek_base_url,
                            **kwargs,
                        )
                    # Pure text format
                    else:
                        return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

                # æŒ‰ç…§native_with_qwen.pyä½¿ç”¨qwen_embedï¼Œä½¿ç”¨EmbeddingFuncåŒ…è£…å™¨
                try:
                    sys.path.append("/home/ragsvr/projects/ragsystem/RAG-Anything")
                    from simple_qwen_embed import qwen_embed
                    embedding_func = EmbeddingFunc(
                        embedding_dim=1024,  # Qwen3-Embedding-0.6Bçš„ç»´åº¦
                        max_token_size=512,   # Qwenæ¨¡å‹çš„æœ€å¤§tokené•¿åº¦
                        func=qwen_embed,      # ä½¿ç”¨æˆ‘ä»¬çš„æœ¬åœ°QwenåµŒå…¥å‡½æ•°
                    )
                    logger.info("âœ… ä½¿ç”¨æœ¬åœ°Qwen3åµŒå…¥æ¨¡å‹")
                except ImportError:
                    logger.warning("âš ï¸ qwen_embedä¸å¯ç”¨ï¼Œä½¿ç”¨OpenAIåµŒå…¥")
                    embedding_func = EmbeddingFunc(
                        embedding_dim=1536,
                        max_token_size=8192,
                        func=openai_embed,
                    )
            else:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°DeepSeek APIå¯†é’¥ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿ")
                # åˆ›å»ºæœ¬åœ°å‡½æ•°ä½œä¸ºåå¤‡
                async def local_llm_func(prompt, **kwargs):
                    return f"æœ¬åœ°å›ç­”ï¼š{prompt[:50]}..."
                
                def local_embed_func(text, **kwargs):
                    import hashlib
                    import numpy as np
                    hash_obj = hashlib.md5(text.encode())
                    seed = int(hash_obj.hexdigest()[:8], 16)
                    np.random.seed(seed)
                    return np.random.rand(1536).tolist()
                
                # æœ¬åœ°visionæ¨¡æ‹Ÿå‡½æ•°
                def local_vision_func(prompt, **kwargs):
                    return f"æœ¬åœ°è§†è§‰æ¨¡æ‹Ÿå›ç­”ï¼š{prompt[:50]}..."
                
                llm_model_func = local_llm_func
                vision_model_func = local_vision_func
                embedding_func = EmbeddingFunc(
                    embedding_dim=1536,
                    max_token_size=8192,
                    func=local_embed_func,
                )
            
            # åˆ›å»ºRAGå®ä¾‹ï¼Œä¸native_with_qwen.pyä¿æŒä¸€è‡´
            rag_instance = RAGAnything(
                config=config,
                llm_model_func=llm_model_func,
                vision_model_func=vision_model_func,
                embedding_func=embedding_func
            )
            
            # ç¡®ä¿RAGå·²åˆå§‹åŒ–
            await rag_instance._ensure_lightrag_initialized()
            
            logger.info("âœ… RAGå®ä¾‹åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ RAGå®ä¾‹åˆå§‹åŒ–å¤±è´¥: {e}")
            # åˆå§‹åŒ–å¤±è´¥æ—¶è¿”å›æ¨¡æ‹Ÿå¯¹è±¡
            return {"status": "init_failed", "message": f"RAGåˆå§‹åŒ–å¤±è´¥: {e}"}
    
    return rag_instance

# å¥åº·æ£€æŸ¥ - å…¼å®¹LightRAG WebUIæ ¼å¼
@app.get("/health", response_model=LightragStatus)
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ - å…¼å®¹LightRAG WebUI"""
    try:
        rag = await get_rag_instance()
        
        # è·å–å·¥ä½œç›®å½•
        working_dir = "/home/ragsvr/projects/ragsystem/RAG-Anything/rag_storage"
        if hasattr(rag, 'config') and hasattr(rag.config, 'working_dir'):
            working_dir = rag.config.working_dir
        
        # æ„å»ºé…ç½®ä¿¡æ¯
        deepseek_api_key = os.getenv("DEEPSEEK_API_KEY") or os.getenv("LLM_BINDING_API_KEY")
        configuration = {
            "llm_binding": "deepseek",
            "llm_binding_host": os.getenv("LLM_BINDING_HOST", "https://api.deepseek.com/v1"),
            "llm_model": "deepseek-chat",
            "embedding_binding": "qwen_local",
            "embedding_binding_host": "local",
            "embedding_model": "Qwen3-Embedding-0.6B",
            "max_tokens": 4000,
            "kv_storage": "json",
            "doc_status_storage": "json", 
            "graph_storage": "json",
            "vector_storage": "json",
            "workspace": working_dir,
            "summary_language": "zh_CN",
            "force_llm_summary_on_merge": True,
            "max_parallel_insert": 4,
            "max_async": 16,
            "embedding_func_max_async": 16,
            "embedding_batch_num": 32,
            "cosine_threshold": 0.2,
            "min_rerank_score": 0.0,
            "related_chunk_number": 10
        }
        
        return LightragStatus(
            status="healthy",
            working_directory=working_dir,
            input_directory=working_dir,
            configuration=configuration,
            pipeline_busy=False,
            core_version="1.0.0",
            api_version="1.0.0", 
            auth_mode="disabled",
            webui_title="RAG-Anything WebUI",
            webui_description="å¤šæ¨¡æ€RAGç³»ç»ŸWebç•Œé¢"
        )
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return LightragStatus(
            status="error",
            working_directory="./rag_storage",
            configuration={},
            pipeline_busy=False
        )

# ç³»ç»ŸçŠ¶æ€
@app.get("/api/system/status", response_model=SystemStatus)
async def get_system_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    try:
        rag = await get_rag_instance()
        
        # æ¨¡æ‹Ÿç³»ç»ŸæŒ‡æ ‡
        metrics = {
            "cpu_usage": 45.2,
            "memory_usage": 68.1,
            "disk_usage": 32.4,
            "gpu_usage": 78.3
        }
        
        # æ¨¡æ‹ŸæœåŠ¡çŠ¶æ€
        services = {
            "lightrag": {"status": "running", "uptime": "2d 14h"},
            "mineru": {"status": "running", "uptime": "2d 14h"},
            "vector_db": {"status": "running", "uptime": "2d 14h"},
            "multimodal": {"status": "running", "uptime": "1d 8h"},
            "web_api": {"status": "running", "uptime": "2d 14h"}
        }
        
        # è·å–çœŸå®çš„RAGç»Ÿè®¡æ•°æ®
        if isinstance(rag, dict):  # æ¨¡æ‹Ÿæ¨¡å¼
            processing_stats = {
                "documents_processed": 0,
                "queries_handled": 0,
                "error_rate": 0.0,
                "avg_response_time": 0.0
            }
        else:
            # ä»RAGå®ä¾‹è·å–çœŸå®ç»Ÿè®¡ï¼Œé¿å…ç›´æ¥è®¿é—®.dataå±æ€§
            try:
                # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼è·å–ç»Ÿè®¡ä¿¡æ¯
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨lightragå®ä¾‹
                if hasattr(rag, 'lightrag') and rag.lightrag:
                    # å°è¯•é€šè¿‡å®‰å…¨çš„æ–¹æ³•è·å–ç»Ÿè®¡ä¿¡æ¯
                    entities_count = 0
                    relationships_count = 0 
                    chunks_count = 0
                    documents_processed = 1  # å·²çŸ¥è‡³å°‘å¤„ç†äº†ä¸€ä¸ªæ–‡æ¡£
                    llm_cache_size = 0
                    
                    # å°è¯•ä»å·¥ä½œç›®å½•è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ›´å®‰å…¨çš„æ–¹æ³•ï¼‰
                    try:
                        working_dir = config.working_dir
                        if os.path.exists(working_dir):
                            # è®¡ç®—å·²å¤„ç†çš„æ–‡æ¡£æ•°é‡
                            entities_file = os.path.join(working_dir, "entities.jsonl")
                            relations_file = os.path.join(working_dir, "relationships.jsonl") 
                            chunks_file = os.path.join(working_dir, "chunks.jsonl")
                            
                            if os.path.exists(entities_file):
                                with open(entities_file, 'r', encoding='utf-8') as f:
                                    entities_count = sum(1 for _ in f)
                            
                            if os.path.exists(relations_file):
                                with open(relations_file, 'r', encoding='utf-8') as f:
                                    relationships_count = sum(1 for _ in f)
                                    
                            if os.path.exists(chunks_file):
                                with open(chunks_file, 'r', encoding='utf-8') as f:
                                    chunks_count = sum(1 for _ in f)
                    except Exception as fe:
                        logger.debug(f"ä»æ–‡ä»¶è·å–ç»Ÿè®¡å¤±è´¥: {fe}")
                        # ä½¿ç”¨é»˜è®¤å€¼
                        pass
                else:
                    # RAGå®ä¾‹ä¸å®Œæ•´ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    entities_count = 0
                    relationships_count = 0
                    chunks_count = 0
                    documents_processed = 0
                    llm_cache_size = 0
                
                processing_stats = {
                    "documents_processed": documents_processed,
                    "queries_handled": llm_cache_size,
                    "entities_count": entities_count,
                    "relationships_count": relationships_count,
                    "knowledge_blocks": chunks_count,
                    "error_rate": 0.0,
                    "avg_response_time": 2.3
                }
                logger.info(f"RAGç»Ÿè®¡: æ–‡æ¡£={documents_processed}, chunks={chunks_count}, å®ä½“={entities_count}, å…³ç³»={relationships_count}")
            except Exception as e:
                logger.warning(f"è·å–RAGç»Ÿè®¡å¤±è´¥: {e}")
                # å®Œå…¨å¤±è´¥æ—¶ä½¿ç”¨åˆç†çš„é»˜è®¤å€¼
                processing_stats = {
                    "documents_processed": 1,  # å‡è®¾è‡³å°‘æœ‰ä¸€ä¸ªæ–‡æ¡£
                    "queries_handled": 0,
                    "entities_count": 0,
                    "relationships_count": 0,
                    "knowledge_blocks": 0,
                    "error_rate": 0.0,
                    "avg_response_time": 2.3
                }
        
        return SystemStatus(
            status="healthy",
            services=services,
            metrics=metrics,
            processing_stats=processing_stats
        )
    except Exception as e:
        logger.error(f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# é…ç½®ç®¡ç†
@app.get("/api/config")
async def get_configuration():
    """è·å–å½“å‰é…ç½®"""
    try:
        rag = await get_rag_instance()
        
        if isinstance(rag, dict):  # æ¨¡æ‹Ÿæ¨¡å¼
            # è¿”å›æ¨¡æ‹Ÿé…ç½®
            config_info = {
                "parser": {
                    "parser": "mineru",
                    "parse_method": "auto", 
                    "max_concurrent_files": 4
                },
                "multimodal": {
                    "enable_image_processing": True,
                    "enable_table_processing": True,
                    "enable_equation_processing": True
                },
                "llm": {
                    "model_name": "gpt-4",
                    "temperature": 0.7,
                    "max_tokens": 4000,
                    "api_key_configured": bool(os.getenv("OPENAI_API_KEY"))
                },
                "storage": {
                    "working_dir": "./rag_storage",
                    "vector_db_type": "chromadb", 
                    "chunk_size": 500
                },
                "system": {
                    "rag_available": RAG_AVAILABLE,
                    "mode": "demo" if not RAG_AVAILABLE else "production"
                }
            }
        else:
            # è·å–çœŸå®é…ç½®
            config_info = rag.get_config_info()
            config_info["system"] = {
                "rag_available": RAG_AVAILABLE,
                "mode": "production",
                "api_key_configured": bool(os.getenv("OPENAI_API_KEY"))
            }
            
        return config_info
    except Exception as e:
        logger.error(f"è·å–é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/config")
async def update_configuration(config_update: ConfigUpdate):
    """æ›´æ–°é…ç½®"""
    try:
        rag = await get_rag_instance()
        
        # æ›´æ–°é…ç½®
        rag.update_config(**config_update.settings)
        
        return {"message": "é…ç½®æ›´æ–°æˆåŠŸ", "section": config_update.section}
    except Exception as e:
        logger.error(f"æ›´æ–°é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# æ–‡æ¡£ä¸Šä¼ å’Œå¤„ç† - å…¼å®¹LightRAG WebUI
class DocActionResponse(BaseModel):
    status: str = "success"  # success, partial_success, failure, duplicated
    message: str
    track_id: Optional[str] = None

@app.post("/documents/upload")
async def upload_document_lightrag_compatible(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...)
):
    """æ–‡æ¡£ä¸Šä¼  - å…¼å®¹LightRAG WebUIæ ¼å¼"""
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        file_id = Path(tmp_file_path).stem
        
        # æ·»åŠ åå°å¤„ç†ä»»åŠ¡
        background_tasks.add_task(
            process_document_background,
            tmp_file_path,
            file.filename,
            "mineru",
            "auto"
        )
        
        return DocActionResponse(
            status="success",
            message=f"æ–‡ä»¶ {file.filename} ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨å¤„ç†ä¸­...",
            track_id=file_id
        )
    except Exception as e:
        logger.error(f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {e}")
        return DocActionResponse(
            status="failure",
            message=f"ä¸Šä¼ å¤±è´¥: {str(e)}"
        )

@app.post("/api/documents/upload", response_model=FileUploadResponse)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    parser: str = Form("mineru"),
    parse_method: str = Form("auto")
):
    """ä¸Šä¼ æ–‡æ¡£"""
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name
        
        file_id = Path(tmp_file_path).stem
        
        # æ·»åŠ åå°å¤„ç†ä»»åŠ¡
        background_tasks.add_task(
            process_document_background,
            tmp_file_path,
            file.filename,
            parser,
            parse_method
        )
        
        return FileUploadResponse(
            file_id=file_id,
            filename=file.filename,
            size=len(content),
            status="uploading",
            message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨å¤„ç†ä¸­..."
        )
    except Exception as e:
        logger.error(f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

async def process_document_background(
    file_path: str,
    filename: str,
    parser: str,
    parse_method: str
):
    """åå°å¤„ç†æ–‡æ¡£"""
    try:
        rag = await get_rag_instance()
        
        # å¤„ç†æ–‡æ¡£ - æŒ‰ç…§åŸç”Ÿç¤ºä¾‹çš„æ­£ç¡®æ ¼å¼
        output_dir = "/home/ragsvr/projects/ragsystem/web-api/output"
        os.makedirs(output_dir, exist_ok=True)
        
        await rag.process_document_complete(
            file_path=file_path,
            output_dir=output_dir, 
            parse_method=parse_method
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(file_path)
        
        logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {filename}")
    except Exception as e:
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {filename}, é”™è¯¯: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.unlink(file_path)
        except:
            pass

# æ–‡æ¡£ç®¡ç† - å…¼å®¹LightRAG WebUI
class DocsStatusesResponse(BaseModel):
    statuses: Dict[str, List[Dict[str, Any]]]

@app.get("/documents")
async def get_documents_lightrag_compatible():
    """è·å–æ–‡æ¡£çŠ¶æ€ - å…¼å®¹LightRAG WebUIæ ¼å¼"""
    try:
        # ä»å®é™…æ•°æ®åº“è¯»å–æ–‡æ¡£çŠ¶æ€
        rag_storage_path = "/home/ragsvr/projects/ragsystem/RAG-Anything/rag_storage"
        doc_status_file = os.path.join(rag_storage_path, "kv_store_doc_status.json")
        
        processed_docs = []
        pending_docs = []
        processing_docs = []
        failed_docs = []
        
        if os.path.exists(doc_status_file):
            try:
                with open(doc_status_file, 'r', encoding='utf-8') as f:
                    doc_statuses = json.load(f)
                
                for doc_id, doc_info in doc_statuses.items():
                    doc_data = {
                        "id": doc_id,
                        "content_summary": doc_info.get("content_summary", "")[:200] + "..." if len(doc_info.get("content_summary", "")) > 200 else doc_info.get("content_summary", ""),
                        "content_length": doc_info.get("content_length", 0),
                        "status": doc_info.get("status", "unknown"),
                        "created_at": doc_info.get("created_at", ""),
                        "updated_at": doc_info.get("updated_at", ""),
                        "track_id": doc_info.get("track_id", ""),
                        "chunks_count": doc_info.get("chunks_count", 0),
                        "file_path": doc_info.get("file_path", ""),
                        "multimodal_processed": doc_info.get("multimodal_processed", False)
                    }
                    
                    status = doc_info.get("status", "unknown")
                    if status == "processed":
                        processed_docs.append(doc_data)
                    elif status == "pending":
                        pending_docs.append(doc_data)
                    elif status == "processing":
                        processing_docs.append(doc_data)
                    elif status == "failed":
                        failed_docs.append(doc_data)
                
                logger.info(f"ä»æ•°æ®åº“åŠ è½½æ–‡æ¡£: processed={len(processed_docs)}, pending={len(pending_docs)}, processing={len(processing_docs)}, failed={len(failed_docs)}")
            except Exception as e:
                logger.error(f"è¯»å–æ–‡æ¡£çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}")
        else:
            logger.warning(f"æ–‡æ¡£çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: {doc_status_file}")
        
        return DocsStatusesResponse(
            statuses={
                "processed": processed_docs,
                "pending": pending_docs,
                "processing": processing_docs,
                "failed": failed_docs
            }
        )
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£å¤±è´¥: {e}")
        return DocsStatusesResponse(statuses={"processed": [], "pending": [], "processing": [], "failed": []})

@app.get("/api/documents")
async def list_documents():
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    try:
        rag = await get_rag_instance()
        
        # è¿™é‡Œåº”è¯¥ä»RAGç³»ç»Ÿè·å–å®é™…çš„æ–‡æ¡£åˆ—è¡¨
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
        documents = [
            {
                "id": "doc1",
                "name": "research_paper.pdf",
                "status": "completed",
                "upload_time": "2024-01-15T10:30:00",
                "chunks": 45,
                "size": 2048576
            },
            {
                "id": "doc2",
                "name": "product_manual.docx",
                "status": "completed",
                "upload_time": "2024-01-15T11:15:00",
                "chunks": 32,
                "size": 1536000
            }
        ]
        
        return {"documents": documents, "total": len(documents)}
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# æŸ¥è¯¢æ¥å£ - å…¼å®¹LightRAG WebUI
class LightRAGQueryRequest(BaseModel):
    query: str
    mode: str = "hybrid"  # naive, local, global, hybrid, mix, bypass
    only_need_context: Optional[bool] = False
    only_need_prompt: Optional[bool] = False
    response_type: Optional[str] = "Multiple Paragraphs"
    stream: Optional[bool] = False
    top_k: Optional[int] = 10
    chunk_top_k: Optional[int] = 5
    max_entity_tokens: Optional[int] = 2000
    max_relation_tokens: Optional[int] = 2000
    max_total_tokens: Optional[int] = 8000
    conversation_history: Optional[List[Dict[str, str]]] = None
    history_turns: Optional[int] = 3
    user_prompt: Optional[str] = None
    enable_rerank: Optional[bool] = True

class LightRAGQueryResponse(BaseModel):
    response: str

@app.post("/query", response_model=LightRAGQueryResponse)
async def query_lightrag_compatible(query_request: LightRAGQueryRequest):
    """æŸ¥è¯¢æ¥å£ - å…¼å®¹LightRAG WebUI"""
    try:
        rag = await get_rag_instance()
        
        if isinstance(rag, dict):  # æ¨¡æ‹Ÿæ¨¡å¼æˆ–åˆå§‹åŒ–å¤±è´¥
            result = f"[æ¨¡æ‹Ÿæ¨¡å¼] è¿™æ˜¯å¯¹æŸ¥è¯¢ '{query_request.query}' çš„æ¨¡æ‹Ÿå›ç­”ã€‚æŸ¥è¯¢æ¨¡å¼: {query_request.mode}"
        else:
            # çœŸå®RAGæŸ¥è¯¢
            try:
                result = await rag.aquery(
                    query_request.query,
                    mode=query_request.mode,
                    vlm_enhanced=False
                )
            except Exception as e:
                logger.error(f"RAGæŸ¥è¯¢å¤±è´¥: {e}")
                result = f"æŸ¥è¯¢å¤±è´¥: {str(e)}"
        
        return LightRAGQueryResponse(response=result)
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/query", response_model=QueryResponse)
async def query_rag(query_request: QueryRequest):
    """æ‰§è¡ŒRAGæŸ¥è¯¢"""
    try:
        rag = await get_rag_instance()
        
        start_time = asyncio.get_event_loop().time()
        
        # æ‰§è¡ŒæŸ¥è¯¢
        if isinstance(rag, dict):  # æ¨¡æ‹Ÿæ¨¡å¼æˆ–åˆå§‹åŒ–å¤±è´¥
            result = f"[æ¨¡æ‹Ÿæ¨¡å¼] è¿™æ˜¯å¯¹æŸ¥è¯¢ '{query_request.query}' çš„æ¨¡æ‹Ÿå›ç­”ã€‚æŸ¥è¯¢æ¨¡å¼: {query_request.mode}"
            if query_request.multimodal_content:
                result += f"\\n\\næ£€æµ‹åˆ° {len(query_request.multimodal_content)} ä¸ªå¤šæ¨¡æ€å†…å®¹é¡¹ã€‚"
        else:
            # çœŸå®RAGæŸ¥è¯¢
            try:
                if query_request.multimodal_content:
                    result = await rag.aquery_with_multimodal(
                        query_request.query,
                        multimodal_content=query_request.multimodal_content,
                        mode=query_request.mode,
                        vlm_enhanced=False
                    )
                else:
                    result = await rag.aquery(
                        query_request.query,
                        mode=query_request.mode,
                        vlm_enhanced=False
                    )
            except Exception as e:
                logger.error(f"RAGæŸ¥è¯¢å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                result = f"æŸ¥è¯¢å¤±è´¥: {str(e)}"
        
        end_time = asyncio.get_event_loop().time()
        processing_time = end_time - start_time
        
        query_id = f"query_{int(datetime.now().timestamp())}"
        
        return QueryResponse(
            result=result,
            query_id=query_id,
            timestamp=datetime.now(),
            mode=query_request.mode,
            processing_time=processing_time
        )
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/query/history")
async def get_query_history(limit: int = 10):
    """è·å–æŸ¥è¯¢å†å²"""
    try:
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“æˆ–ç¼“å­˜è·å–å®é™…çš„æŸ¥è¯¢å†å²
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
        history = [
            {
                "id": "query_1",
                "query": "æœºå™¨å­¦ä¹ çš„ä¸»è¦åº”ç”¨é¢†åŸŸæœ‰å“ªäº›ï¼Ÿ",
                "result": "æ ¹æ®æ–‡æ¡£åˆ†æï¼Œæœºå™¨å­¦ä¹ çš„ä¸»è¦åº”ç”¨é¢†åŸŸåŒ…æ‹¬...",
                "timestamp": "2024-01-15T14:30:00",
                "mode": "hybrid",
                "processing_time": 2.1
            }
        ]
        
        return {"history": history[:limit], "total": len(history)}
    except Exception as e:
        logger.error(f"è·å–æŸ¥è¯¢å†å²å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# å¯åŠ¨è¯´æ˜
@app.get("/")
async def root():
    """APIæ ¹è·¯å¾„"""
    return {
        "message": "RAG-Anything Web API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health"
    }

if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
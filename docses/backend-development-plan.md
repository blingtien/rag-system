# åç«¯å¼€å‘è®¡åˆ’

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

RAG-Anythingåç«¯å¼€å‘ä¸»è¦å›´ç»•web-apiæœåŠ¡çš„å®Œå–„ï¼Œå®ç°RAG-Anythingæ ¸å¿ƒåŠŸèƒ½ä¸LightRAG WebUIçš„æ— ç¼é›†æˆã€‚

### æŠ€æœ¯æ¶æ„
- **æ ¸å¿ƒç³»ç»Ÿ**: RAG-Anything v1.2.7 (åŸºäºLightRAG)
- **APIæ¡†æ¶**: FastAPI 0.104+
- **æ–‡æ¡£è§£æ**: MinerU 2.0
- **å‘é‡å­˜å‚¨**: æœ¬åœ°JSONå­˜å‚¨
- **åµŒå…¥æ¨¡å‹**: Qwen3-Embedding-0.6B
- **LLM**: DeepSeek API

## ğŸ“‹ è¯¦ç»†å¼€å‘ä»»åŠ¡

### Phase 1: APIæ ‡å‡†åŒ–ä¸å…¼å®¹æ€§ (ğŸ”¥é«˜ä¼˜å…ˆçº§)

#### 1.1 LightRAG APIå…¼å®¹æ€§å®Œå–„
**é¢„ä¼°æ—¶é—´**: 3å¤©
**è´Ÿè´£æ¨¡å—**: `web-api/main.py`

**æ ¸å¿ƒæ¥å£æ ‡å‡†åŒ–**:
```python
# éœ€è¦å®Œå–„çš„æ ¸å¿ƒæ¥å£
1. GET /health - ç³»ç»Ÿå¥åº·æ£€æŸ¥
   å½“å‰çŠ¶æ€: âœ… åŸºç¡€å®ç°å®Œæˆ
   éœ€è¦å®Œå–„: 
   - æ·»åŠ è¯¦ç»†çš„ç³»ç»Ÿç»„ä»¶çŠ¶æ€æ£€æŸ¥
   - åŒ…å«RAG-Anythingç‰¹æœ‰çš„é…ç½®ä¿¡æ¯
   - æ·»åŠ pipeline_busyçŠ¶æ€æŒ‡ç¤º

2. POST /documents/upload - æ–‡æ¡£ä¸Šä¼ å¤„ç†
   å½“å‰çŠ¶æ€: âœ… åŸºç¡€åŠŸèƒ½å¯ç”¨
   éœ€è¦å®Œå–„:
   - å¤šæ–‡ä»¶æ‰¹é‡ä¸Šä¼ æ”¯æŒ
   - æ–‡ä»¶ä¸Šä¼ çŠ¶æ€åé¦ˆ
   - æ–‡ä»¶ç±»å‹éªŒè¯å’Œé”™è¯¯å¤„ç†

3. GET /documents - æ–‡æ¡£çŠ¶æ€åˆ—è¡¨
   å½“å‰çŠ¶æ€: ğŸ”„ å·²ä¿®å¤åŸºç¡€æ•°æ®è¿”å›
   éœ€è¦å®Œå–„:
   - æ–‡æ¡£è§£æè¿‡ç¨‹çš„å®æ—¶çŠ¶æ€å±•ç¤ºï¼ˆè§£æè€—æ—¶è¾ƒé•¿ï¼‰
   - WebSocketå®æ—¶çŠ¶æ€æ¨é€æœºåˆ¶
   - è§£æè¿›åº¦è¯¦ç»†ä¿¡æ¯ï¼ˆé¡µé¢æ•°ã€å¤„ç†é˜¶æ®µç­‰ï¼‰
   - åˆ†é¡µå’Œæ’åºåŠŸèƒ½
   - çŠ¶æ€ç­›é€‰åŠŸèƒ½

4. POST /query - æ™ºèƒ½æŸ¥è¯¢æ¥å£
   å½“å‰çŠ¶æ€: âœ… åŸºç¡€æŸ¥è¯¢åŠŸèƒ½å¯ç”¨
   éœ€è¦å®Œå–„:
   - æµå¼å“åº”æ”¯æŒ
   - å¤šæ¨¡æ€æŸ¥è¯¢å¤„ç†
   - æŸ¥è¯¢å†å²ç®¡ç†

5. GET /graph - çŸ¥è¯†å›¾è°±æ•°æ®
   å½“å‰çŠ¶æ€: âŒ éœ€è¦å®ç°
   éœ€è¦å®ç°:
   - å›¾è°±æ•°æ®æ ¼å¼è½¬æ¢
   - èŠ‚ç‚¹å’Œè¾¹çš„è¿‡æ»¤
   - å¤§å‹å›¾è°±çš„åˆ†é¡µåŠ è½½
```

**å…·ä½“å®ç°ä»»åŠ¡**:

```python
# 1. å®Œå–„å¥åº·æ£€æŸ¥æ¥å£
@app.get("/health")
async def get_health():
    """
    è¿”å›LightRAGå…¼å®¹çš„ç³»ç»ŸçŠ¶æ€
    éœ€è¦åŒ…å«: é…ç½®ä¿¡æ¯ã€pipelineçŠ¶æ€ã€ç‰ˆæœ¬ä¿¡æ¯
    """
    return LightragStatus(
        status="healthy",
        working_directory=str(rag_storage_path),
        configuration={...},  # è¯¦ç»†é…ç½®ä¿¡æ¯
        pipeline_busy=is_processing,  # å®æ—¶å¤„ç†çŠ¶æ€
        core_version="1.2.7",
        api_version="1.0.0"
    )

# 2. å®ç°çŸ¥è¯†å›¾è°±æ¥å£
@app.get("/graph")
async def get_graph(limit: int = 1000):
    """
    è¿”å›LightRAGå…¼å®¹çš„å›¾è°±æ•°æ®æ ¼å¼
    åŒ…å«èŠ‚ç‚¹ã€è¾¹ã€å±æ€§ä¿¡æ¯
    """
    # è¯»å–RAG-Anythingçš„å›¾è°±æ•°æ®
    # è½¬æ¢ä¸ºLightRAGæ ¼å¼
    # è¿”å›æ ‡å‡†åŒ–çš„å›¾è°±ç»“æ„

# 3. ä¼˜åŒ–æ–‡æ¡£æ¥å£
@app.get("/documents")
async def get_documents_paginated(
    page: int = 1,
    page_size: int = 20,
    status_filter: Optional[str] = None,
    sort_field: str = "updated_at",
    sort_direction: str = "desc"
):
    """
    æ”¯æŒåˆ†é¡µã€æ’åºã€ç­›é€‰çš„æ–‡æ¡£åˆ—è¡¨
    å…¼å®¹LightRAG WebUIçš„è°ƒç”¨æ ¼å¼
    """
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰æ¥å£å“åº”æ ¼å¼ä¸LightRAG WebUIå…¼å®¹
- [ ] APIæ–‡æ¡£å®Œæ•´ï¼ŒåŒ…å«Schemaå®šä¹‰
- [ ] é”™è¯¯å“åº”æ ¼å¼ç»Ÿä¸€æ ‡å‡†åŒ–

#### 1.2 æ•°æ®æ ¼å¼æ ‡å‡†åŒ–
**é¢„ä¼°æ—¶é—´**: 2å¤©
**è´Ÿè´£æ¨¡å—**: Response Models

**ç»Ÿä¸€å“åº”æ ¼å¼**:
```python
# 1. æ–‡æ¡£çŠ¶æ€æ ¼å¼æ ‡å‡†åŒ–
class DocStatusResponse(BaseModel):
    id: str
    content_summary: str
    content_length: int
    status: Literal["processed", "processing", "pending", "failed"]
    created_at: str
    updated_at: str
    track_id: str
    chunks_count: int
    file_path: str
    error_msg: Optional[str] = None
    multimodal_processed: bool = False

# 2. æŸ¥è¯¢å“åº”æ ¼å¼æ ‡å‡†åŒ–
class QueryResponse(BaseModel):
    response: str
    query_id: str
    mode: str
    conversation_history: List[Dict[str, str]]
    sources: List[Dict[str, Any]]  # æ–°å¢ï¼šæºæ–‡æ¡£å¼•ç”¨
    confidence_score: float  # æ–°å¢ï¼šç½®ä¿¡åº¦è¯„åˆ†
    processing_time: float

# 3. çŸ¥è¯†å›¾è°±æ ¼å¼æ ‡å‡†åŒ–
class GraphNode(BaseModel):
    id: str
    labels: List[str]
    properties: Dict[str, Any]

class GraphEdge(BaseModel):
    id: str
    source: str
    target: str
    type: str
    properties: Dict[str, Any]

class GraphResponse(BaseModel):
    nodes: List[GraphNode]
    edges: List[GraphEdge]
    total_nodes: int
    total_edges: int
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ•°æ®æ ¼å¼å®Œå…¨å…¼å®¹LightRAG WebUI
- [ ] åŒ…å«å¿…è¦çš„å…ƒæ•°æ®ä¿¡æ¯
- [ ] æ”¯æŒæ‰©å±•å­—æ®µï¼Œä¿æŒå‘åå…¼å®¹

#### 1.3 é…ç½®ç³»ç»Ÿä¼˜åŒ–
**é¢„ä¼°æ—¶é—´**: 1å¤©
**è´Ÿè´£æ¨¡å—**: Configuration Management

**ç»Ÿä¸€é…ç½®ç®¡ç†**:
```python
# 1. ç¯å¢ƒå˜é‡æ ‡å‡†åŒ–
WORKING_DIR = os.getenv("WORKING_DIR", "./rag_storage")
OUTPUT_DIR = os.getenv("OUTPUT_DIR", "./output")
PARSER = os.getenv("PARSER", "mineru")
PARSE_METHOD = os.getenv("PARSE_METHOD", "auto")

# 2. RAG-Anythingé…ç½®é€‚é…
config = RAGAnythingConfig(
    working_dir=WORKING_DIR,
    enable_image_processing=True,
    enable_table_processing=True,
    enable_equation_processing=True,
    max_concurrent_files=4
)

# 3. é…ç½®æ¥å£å®ç°
@app.get("/config")
async def get_config():
    """è¿”å›å½“å‰ç³»ç»Ÿé…ç½®"""
    
@app.post("/config")
async def update_config(config_update: ConfigUpdate):
    """æ›´æ–°ç³»ç»Ÿé…ç½®"""
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] é…ç½®å˜æ›´ç«‹å³ç”Ÿæ•ˆ
- [ ] é…ç½®æŒä¹…åŒ–å­˜å‚¨
- [ ] æ”¯æŒé…ç½®éªŒè¯å’Œå›æ»š

### Phase 2: é”™è¯¯å¤„ç†ä¸ç¨³å®šæ€§ (ğŸ”¥é«˜ä¼˜å…ˆçº§)

#### 2.1 å¼‚å¸¸å¤„ç†å¢å¼º
**é¢„ä¼°æ—¶é—´**: 2å¤©
**è´Ÿè´£æ¨¡å—**: Error Handling

**æ–‡ä»¶ä¸Šä¼ é”™è¯¯å¤„ç†**:
```python
# 1. æ–‡ä»¶éªŒè¯å’Œé”™è¯¯å¤„ç†
async def validate_upload_file(file: UploadFile):
    """
    æ–‡ä»¶ä¸Šä¼ å‰éªŒè¯
    - æ–‡ä»¶å¤§å°é™åˆ¶ (100MB)
    - æ–‡ä»¶ç±»å‹æ£€æŸ¥
    - æ–‡ä»¶åå®‰å…¨æ€§æ£€æŸ¥
    """
    if file.size > 100 * 1024 * 1024:
        raise HTTPException(400, "æ–‡ä»¶å¤§å°è¶…è¿‡100MBé™åˆ¶")
    
    if not is_supported_file_type(file.filename):
        raise HTTPException(400, f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.filename}")

# 2. è§£æå¤±è´¥æ¢å¤æœºåˆ¶
async def handle_parsing_failure(file_path: str, error: Exception):
    """
    è§£æå¤±è´¥æ—¶çš„æ¢å¤ç­–ç•¥
    - è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
    - å°è¯•å¤‡ç”¨è§£ææ–¹æ³•
    - ä¿å­˜éƒ¨åˆ†æˆåŠŸçš„ç»“æœ
    """
    logger.error(f"æ–‡æ¡£è§£æå¤±è´¥: {file_path}, é”™è¯¯: {error}")
    
    # å°è¯•å¤‡ç”¨è§£ææ–¹æ³•
    if isinstance(error, MinerUError):
        return await try_docling_parser(file_path)
    
    # æ ‡è®°ä¸ºå¤±è´¥çŠ¶æ€
    await update_doc_status(file_path, "failed", str(error))

# 3. æŸ¥è¯¢è¶…æ—¶å’Œé‡è¯•
async def query_with_retry(query_params: QueryRequest, max_retries: int = 3):
    """
    æŸ¥è¯¢é‡è¯•æœºåˆ¶
    - è¶…æ—¶æ£€æµ‹å’Œå¤„ç†
    - è‡ªåŠ¨é‡è¯•é€»è¾‘
    - é™çº§å¤„ç†ç­–ç•¥
    """
    for attempt in range(max_retries):
        try:
            return await execute_query(query_params)
        except TimeoutError:
            if attempt == max_retries - 1:
                raise HTTPException(408, "æŸ¥è¯¢è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
            await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

**ç³»ç»Ÿèµ„æºä¿æŠ¤**:
```python
# 1. å†…å­˜ä½¿ç”¨ç›‘æ§
async def check_memory_usage():
    """ç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œé˜²æ­¢OOM"""
    memory_percent = psutil.virtual_memory().percent
    if memory_percent > 85:
        raise HTTPException(503, "ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œè¯·ç¨åé‡è¯•")

# 2. å¹¶å‘æ§åˆ¶
semaphore = asyncio.Semaphore(10)  # æœ€å¤§å¹¶å‘å¤„ç†æ•°

async def process_with_semaphore(func, *args):
    async with semaphore:
        return await func(*args)

# 3. è¯·æ±‚é¢‘ç‡é™åˆ¶
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/query")
@limiter.limit("60/minute")  # æ¯åˆ†é’Ÿæœ€å¤š60æ¬¡æŸ¥è¯¢
async def query_endpoint(request: Request, query: QueryRequest):
    """å¸¦é¢‘ç‡é™åˆ¶çš„æŸ¥è¯¢æ¥å£"""
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰å¯èƒ½çš„å¼‚å¸¸éƒ½æœ‰é€‚å½“å¤„ç†
- [ ] é”™è¯¯ä¿¡æ¯å‹å¥½ä¸”å…·ä½“
- [ ] ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹ç¨³å®šè¿è¡Œ

#### 2.2 æ•°æ®éªŒè¯å’Œå®‰å…¨
**é¢„ä¼°æ—¶é—´**: 2å¤©
**è´Ÿè´£æ¨¡å—**: Security & Validation

**è¾“å…¥éªŒè¯å¼ºåŒ–**:
```python
# 1. æŸ¥è¯¢è¾“å…¥æ¸…ç†
def sanitize_query_input(query: str) -> str:
    """
    æ¸…ç†æŸ¥è¯¢è¾“å…¥ï¼Œé˜²æ­¢æ³¨å…¥æ”»å‡»
    - è¿‡æ»¤ç‰¹æ®Šå­—ç¬¦
    - é•¿åº¦é™åˆ¶
    - ç¼–ç æ£€æŸ¥
    """
    if len(query) > 10000:
        raise HTTPException(400, "æŸ¥è¯¢å†…å®¹è¿‡é•¿")
    
    # ç§»é™¤æ½œåœ¨å±é™©å­—ç¬¦
    sanitized = re.sub(r'[<>"\']', '', query)
    return sanitized.strip()

# 2. æ–‡ä»¶è·¯å¾„å®‰å…¨æ£€æŸ¥
def validate_file_path(file_path: str) -> str:
    """
    éªŒè¯æ–‡ä»¶è·¯å¾„å®‰å…¨æ€§ï¼Œé˜²æ­¢ç›®å½•éå†
    """
    # è§„èŒƒåŒ–è·¯å¾„
    normalized = os.path.normpath(file_path)
    
    # æ£€æŸ¥æ˜¯å¦åœ¨å…è®¸çš„ç›®å½•å†…
    if not normalized.startswith(ALLOWED_UPLOAD_DIR):
        raise HTTPException(400, "éæ³•æ–‡ä»¶è·¯å¾„")
    
    return normalized

# 3. APIè°ƒç”¨è®¤è¯ (å¯é€‰)
def verify_api_key(api_key: str = Header(None)):
    """
    APIå¯†é’¥éªŒè¯ (å¦‚æœéœ€è¦)
    """
    if REQUIRE_AUTH and api_key != VALID_API_KEY:
        raise HTTPException(401, "æ— æ•ˆçš„APIå¯†é’¥")
```

**æ—¥å¿—å’Œå®¡è®¡**:
```python
# 1. ç»“æ„åŒ–æ—¥å¿—
import structlog

logger = structlog.get_logger()

async def log_api_request(request: Request, response: Response):
    """è®°å½•APIè¯·æ±‚å’Œå“åº”"""
    logger.info(
        "api_request",
        method=request.method,
        url=str(request.url),
        status_code=response.status_code,
        user_agent=request.headers.get("user-agent"),
        timestamp=datetime.utcnow().isoformat()
    )

# 2. æ“ä½œå®¡è®¡æ—¥å¿—
async def log_document_operation(operation: str, file_path: str, user_id: str = None):
    """è®°å½•æ–‡æ¡£æ“ä½œæ—¥å¿—"""
    logger.info(
        "document_operation",
        operation=operation,
        file_path=file_path,
        user_id=user_id,
        timestamp=datetime.utcnow().isoformat()
    )

# 3. æ€§èƒ½ç›‘æ§æ—¥å¿—
async def log_performance_metrics(endpoint: str, duration: float, memory_usage: float):
    """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
    logger.info(
        "performance_metrics",
        endpoint=endpoint,
        duration_ms=duration * 1000,
        memory_mb=memory_usage / 1024 / 1024,
        timestamp=datetime.utcnow().isoformat()
    )
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰è¾“å…¥éƒ½ç»è¿‡éªŒè¯å’Œæ¸…ç†
- [ ] å®‰å…¨æ¼æ´æ‰«æé€šè¿‡
- [ ] æ—¥å¿—è®°å½•å®Œæ•´ä¸”ç»“æ„åŒ–

### Phase 3: æ€§èƒ½ä¼˜åŒ–ä¸æ‰©å±•æ€§ (ğŸ”¶ä¸­ä¼˜å…ˆçº§)

#### 3.1 å¹¶å‘å¤„ç†ä¼˜åŒ–
**é¢„ä¼°æ—¶é—´**: 3å¤©
**è´Ÿè´£æ¨¡å—**: Concurrency & Queue Management

**å¼‚æ­¥æ–‡æ¡£å¤„ç†é˜Ÿåˆ—**:
```python
# 1. å¤„ç†é˜Ÿåˆ—å®ç°
from asyncio import Queue
from dataclasses import dataclass

@dataclass
class ProcessingTask:
    file_path: str
    task_id: str
    priority: int
    created_at: datetime

class DocumentProcessor:
    def __init__(self, max_workers: int = 4):
        self.queue = Queue()
        self.workers = []
        self.max_workers = max_workers
        self.processing_tasks = {}
    
    async def start_workers(self):
        """å¯åŠ¨å¤„ç†å·¥ä½œçº¿ç¨‹"""
        for i in range(self.max_workers):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            self.workers.append(worker)
    
    async def _worker(self, worker_name: str):
        """å¤„ç†å·¥ä½œçº¿ç¨‹"""
        while True:
            task = await self.queue.get()
            try:
                await self._process_document(task)
            except Exception as e:
                logger.error(f"å¤„ç†ä»»åŠ¡å¤±è´¥: {task.task_id}, é”™è¯¯: {e}")
            finally:
                self.queue.task_done()
    
    async def add_task(self, file_path: str, priority: int = 0):
        """æ·»åŠ å¤„ç†ä»»åŠ¡"""
        task = ProcessingTask(
            file_path=file_path,
            task_id=str(uuid.uuid4()),
            priority=priority,
            created_at=datetime.utcnow()
        )
        await self.queue.put(task)
        return task.task_id

# 2. å¹¶å‘æŸ¥è¯¢ä¼˜åŒ–
class QueryProcessor:
    def __init__(self, max_concurrent_queries: int = 10):
        self.semaphore = asyncio.Semaphore(max_concurrent_queries)
        self.active_queries = {}
    
    async def execute_query(self, query_params: QueryRequest):
        """å¹¶å‘å®‰å…¨çš„æŸ¥è¯¢æ‰§è¡Œ"""
        async with self.semaphore:
            query_id = str(uuid.uuid4())
            self.active_queries[query_id] = datetime.utcnow()
            
            try:
                result = await rag_instance.aquery(
                    query_params.query,
                    mode=query_params.mode
                )
                return result
            finally:
                del self.active_queries[query_id]

# 3. èµ„æºæ± ç®¡ç†
class ResourcePool:
    def __init__(self):
        self.rag_instances = []
        self.lock = asyncio.Lock()
    
    async def get_rag_instance(self):
        """è·å–å¯ç”¨çš„RAGå®ä¾‹"""
        async with self.lock:
            if self.rag_instances:
                return self.rag_instances.pop()
            else:
                return await self._create_new_instance()
    
    async def return_rag_instance(self, instance):
        """å½’è¿˜RAGå®ä¾‹åˆ°æ± ä¸­"""
        async with self.lock:
            self.rag_instances.append(instance)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ”¯æŒå¤šæ–‡æ¡£å¹¶å‘å¤„ç†
- [ ] æŸ¥è¯¢å“åº”æ—¶é—´åœ¨é«˜å¹¶å‘ä¸‹ç¨³å®š
- [ ] èµ„æºä½¿ç”¨æ•ˆç‡æå‡50%ä»¥ä¸Š

#### 3.2 ç¼“å­˜ç­–ç•¥å®ç°
**é¢„ä¼°æ—¶é—´**: 3å¤©
**è´Ÿè´£æ¨¡å—**: Caching System

**å¤šå±‚ç¼“å­˜æ¶æ„**:
```python
# 1. æŸ¥è¯¢ç»“æœç¼“å­˜
import hashlib
from typing import Dict, Any, Optional

class QueryCache:
    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.max_size = max_size
        self.ttl = ttl
    
    def _generate_key(self, query: str, mode: str, params: Dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{query}:{mode}:{json.dumps(params, sort_keys=True)}"
        return hashlib.md5(content.encode()).hexdigest()
    
    async def get(self, query: str, mode: str, params: Dict) -> Optional[str]:
        """è·å–ç¼“å­˜ç»“æœ"""
        key = self._generate_key(query, mode, params)
        
        if key in self.cache:
            entry = self.cache[key]
            if datetime.utcnow().timestamp() - entry['timestamp'] < self.ttl:
                return entry['result']
            else:
                del self.cache[key]
        
        return None
    
    async def set(self, query: str, mode: str, params: Dict, result: str):
        """è®¾ç½®ç¼“å­˜ç»“æœ"""
        if len(self.cache) >= self.max_size:
            # LRUæ¸…ç†ç­–ç•¥
            oldest_key = min(self.cache.keys(), 
                           key=lambda k: self.cache[k]['timestamp'])
            del self.cache[oldest_key]
        
        key = self._generate_key(query, mode, params)
        self.cache[key] = {
            'result': result,
            'timestamp': datetime.utcnow().timestamp()
        }

# 2. æ–‡æ¡£è§£æç¼“å­˜
class DocumentCache:
    def __init__(self, cache_dir: str = "./cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
    
    async def get_parsed_content(self, file_path: str, file_hash: str) -> Optional[Dict]:
        """è·å–å·²è§£æçš„æ–‡æ¡£å†…å®¹"""
        cache_file = self.cache_dir / f"{file_hash}.json"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"ç¼“å­˜æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        
        return None
    
    async def set_parsed_content(self, file_hash: str, content: Dict):
        """ç¼“å­˜è§£æç»“æœ"""
        cache_file = self.cache_dir / f"{file_hash}.json"
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(content, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")

# 3. çŸ¥è¯†å›¾è°±ç¼“å­˜
class GraphCache:
    def __init__(self):
        self.graph_cache = None
        self.last_update = None
        self.cache_ttl = 300  # 5åˆ†é’Ÿ
    
    async def get_graph_data(self) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„å›¾è°±æ•°æ®"""
        if (self.graph_cache and self.last_update and 
            datetime.utcnow().timestamp() - self.last_update < self.cache_ttl):
            return self.graph_cache
        
        return None
    
    async def update_graph_cache(self, graph_data: Dict):
        """æ›´æ–°å›¾è°±ç¼“å­˜"""
        self.graph_cache = graph_data
        self.last_update = datetime.utcnow().timestamp()
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] ç¼“å­˜å‘½ä¸­ç‡è¾¾åˆ°70%ä»¥ä¸Š
- [ ] æŸ¥è¯¢å“åº”æ—¶é—´æå‡60%ä»¥ä¸Š
- [ ] ç¼“å­˜å¤±æ•ˆæœºåˆ¶å‡†ç¡®å¯é 

#### 3.3 æ•°æ®åº“ä¼˜åŒ–
**é¢„ä¼°æ—¶é—´**: 2å¤©
**è´Ÿè´£æ¨¡å—**: Data Storage Optimization

**å­˜å‚¨ç»“æ„ä¼˜åŒ–**:
```python
# 1. ç´¢å¼•ç­–ç•¥ä¼˜åŒ–
class DocumentIndex:
    def __init__(self, index_file: str = "document_index.json"):
        self.index_file = index_file
        self.index = self._load_index()
    
    def _load_index(self) -> Dict:
        """åŠ è½½æ–‡æ¡£ç´¢å¼•"""
        if os.path.exists(self.index_file):
            with open(self.index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "documents": {},
            "by_status": {"processed": [], "processing": [], "pending": [], "failed": []},
            "by_date": {},
            "by_type": {}
        }
    
    def add_document(self, doc_id: str, doc_info: Dict):
        """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
        self.index["documents"][doc_id] = doc_info
        
        # æŒ‰çŠ¶æ€ç´¢å¼•
        status = doc_info.get("status", "pending")
        if doc_id not in self.index["by_status"][status]:
            self.index["by_status"][status].append(doc_id)
        
        # æŒ‰æ—¥æœŸç´¢å¼•
        date_key = doc_info.get("created_at", "")[:10]  # YYYY-MM-DD
        if date_key not in self.index["by_date"]:
            self.index["by_date"][date_key] = []
        self.index["by_date"][date_key].append(doc_id)
        
        self._save_index()
    
    def query_documents(self, status: str = None, date_range: tuple = None, 
                       file_type: str = None, page: int = 1, page_size: int = 20):
        """é«˜æ•ˆæŸ¥è¯¢æ–‡æ¡£"""
        doc_ids = list(self.index["documents"].keys())
        
        # çŠ¶æ€ç­›é€‰
        if status and status != "all":
            doc_ids = [doc_id for doc_id in doc_ids 
                      if doc_id in self.index["by_status"].get(status, [])]
        
        # åˆ†é¡µ
        start = (page - 1) * page_size
        end = start + page_size
        
        result_docs = []
        for doc_id in doc_ids[start:end]:
            if doc_id in self.index["documents"]:
                result_docs.append(self.index["documents"][doc_id])
        
        return {
            "documents": result_docs,
            "total_count": len(doc_ids),
            "page": page,
            "page_size": page_size
        }

# 2. æ•°æ®å‹ç¼©ç­–ç•¥
import gzip
import pickle

class CompressedStorage:
    @staticmethod
    def save_compressed(data: Any, file_path: str):
        """å‹ç¼©ä¿å­˜æ•°æ®"""
        with gzip.open(file_path, 'wb') as f:
            pickle.dump(data, f)
    
    @staticmethod
    def load_compressed(file_path: str) -> Any:
        """åŠ è½½å‹ç¼©æ•°æ®"""
        with gzip.open(file_path, 'rb') as f:
            return pickle.load(f)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] å¤§è§„æ¨¡æ•°æ®æŸ¥è¯¢æ€§èƒ½æå‡3å€ä»¥ä¸Š
- [ ] å­˜å‚¨ç©ºé—´ä½¿ç”¨ä¼˜åŒ–50%ä»¥ä¸Š
- [ ] æ•°æ®ä¸€è‡´æ€§ä¿æŒ100%

### Phase 4: ä¼ä¸šçº§åŠŸèƒ½ (ğŸ”·ä½ä¼˜å…ˆçº§)

#### 4.1 ç›‘æ§å’Œå¯è§‚æµ‹æ€§
**é¢„ä¼°æ—¶é—´**: 4å¤©
**è´Ÿè´£æ¨¡å—**: Monitoring & Observability

**æŒ‡æ ‡æ”¶é›†ç³»ç»Ÿ**:
```python
# 1. PrometheusæŒ‡æ ‡é›†æˆ
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# å®šä¹‰æŒ‡æ ‡
REQUEST_COUNT = Counter('api_requests_total', 'Total API requests', ['method', 'endpoint'])
REQUEST_DURATION = Histogram('api_request_duration_seconds', 'Request duration')
PROCESSING_QUEUE_SIZE = Gauge('document_processing_queue_size', 'Documents in processing queue')
ACTIVE_QUERIES = Gauge('active_queries_count', 'Number of active queries')

# 2. æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    
    REQUEST_COUNT.labels(
        method=request.method,
        endpoint=request.url.path
    ).inc()
    
    response = await call_next(request)
    
    REQUEST_DURATION.observe(time.time() - start_time)
    
    return response

# 3. ç³»ç»Ÿèµ„æºç›‘æ§
import psutil

class SystemMonitor:
    def __init__(self):
        self.cpu_usage = Gauge('system_cpu_usage_percent', 'CPU usage percentage')
        self.memory_usage = Gauge('system_memory_usage_percent', 'Memory usage percentage')
        self.disk_usage = Gauge('system_disk_usage_percent', 'Disk usage percentage')
    
    async def update_metrics(self):
        """æ›´æ–°ç³»ç»ŸæŒ‡æ ‡"""
        self.cpu_usage.set(psutil.cpu_percent())
        self.memory_usage.set(psutil.virtual_memory().percent)
        self.disk_usage.set(psutil.disk_usage('/').percent)

# 4. åº”ç”¨æŒ‡æ ‡ç›‘æ§
class ApplicationMonitor:
    def __init__(self):
        self.documents_processed = Counter('documents_processed_total', 'Total processed documents')
        self.queries_executed = Counter('queries_executed_total', 'Total executed queries', ['mode'])
        self.processing_errors = Counter('processing_errors_total', 'Total processing errors', ['type'])
    
    def record_document_processed(self):
        self.documents_processed.inc()
    
    def record_query_executed(self, mode: str):
        self.queries_executed.labels(mode=mode).inc()
    
    def record_processing_error(self, error_type: str):
        self.processing_errors.labels(type=error_type).inc()
```

#### 4.2 é«˜å¯ç”¨æ€§æ”¯æŒ
**é¢„ä¼°æ—¶é—´**: 3å¤©
**è´Ÿè´£æ¨¡å—**: High Availability

**å¥åº·æ£€æŸ¥æœºåˆ¶**:
```python
# 1. æ·±åº¦å¥åº·æ£€æŸ¥
class HealthChecker:
    async def check_rag_system(self) -> Dict[str, Any]:
        """æ£€æŸ¥RAGç³»ç»Ÿå¥åº·çŠ¶æ€"""
        try:
            # æµ‹è¯•ç®€å•æŸ¥è¯¢
            test_result = await rag_instance.aquery("test", mode="naive")
            return {"status": "healthy", "test_query": "passed"}
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}
    
    async def check_storage_system(self) -> Dict[str, Any]:
        """æ£€æŸ¥å­˜å‚¨ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        try:
            # æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å¯è®¿é—®
            storage_path = Path(WORKING_DIR)
            if storage_path.exists() and storage_path.is_dir():
                return {"status": "healthy", "storage_path": str(storage_path)}
            else:
                return {"status": "unhealthy", "error": "Storage path not accessible"}
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}
    
    async def check_model_availability(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§"""
        try:
            # æµ‹è¯•åµŒå…¥æ¨¡å‹
            test_embedding = await embed_func(["test"])
            if test_embedding and len(test_embedding) > 0:
                return {"status": "healthy", "embedding_model": "available"}
            else:
                return {"status": "unhealthy", "error": "Embedding model not responding"}
        except Exception as e:
            return {"status": "unhealthy", "error": str(e)}

# 2. è‡ªåŠ¨æ•…éšœæ¢å¤
class AutoRecovery:
    def __init__(self):
        self.max_retry_attempts = 3
        self.recovery_strategies = {
            "rag_system_failure": self._recover_rag_system,
            "storage_failure": self._recover_storage_system,
            "model_failure": self._recover_model_system
        }
    
    async def _recover_rag_system(self):
        """RAGç³»ç»Ÿæ•…éšœæ¢å¤"""
        logger.info("å°è¯•æ¢å¤RAGç³»ç»Ÿ...")
        try:
            global rag_instance
            rag_instance = await initialize_rag_system()
            return True
        except Exception as e:
            logger.error(f"RAGç³»ç»Ÿæ¢å¤å¤±è´¥: {e}")
            return False
    
    async def _recover_storage_system(self):
        """å­˜å‚¨ç³»ç»Ÿæ•…éšœæ¢å¤"""
        logger.info("å°è¯•æ¢å¤å­˜å‚¨ç³»ç»Ÿ...")
        try:
            # é‡æ–°åˆ›å»ºå¿…è¦çš„ç›®å½•
            Path(WORKING_DIR).mkdir(parents=True, exist_ok=True)
            Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
            return True
        except Exception as e:
            logger.error(f"å­˜å‚¨ç³»ç»Ÿæ¢å¤å¤±è´¥: {e}")
            return False

# 3. æ•°æ®å¤‡ä»½æœºåˆ¶
class BackupManager:
    def __init__(self, backup_dir: str = "./backups"):
        self.backup_dir = Path(backup_dir)
        self.backup_dir.mkdir(exist_ok=True)
    
    async def create_backup(self):
        """åˆ›å»ºæ•°æ®å¤‡ä»½"""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        backup_name = f"rag_backup_{timestamp}"
        backup_path = self.backup_dir / backup_name
        
        try:
            # å‹ç¼©RAGå­˜å‚¨ç›®å½•
            shutil.make_archive(str(backup_path), 'zip', WORKING_DIR)
            logger.info(f"å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_path}.zip")
            
            # æ¸…ç†æ—§å¤‡ä»½ (ä¿ç•™æœ€è¿‘10ä¸ª)
            await self._cleanup_old_backups()
            
        except Exception as e:
            logger.error(f"å¤‡ä»½åˆ›å»ºå¤±è´¥: {e}")
    
    async def _cleanup_old_backups(self):
        """æ¸…ç†æ—§å¤‡ä»½"""
        backup_files = list(self.backup_dir.glob("rag_backup_*.zip"))
        backup_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        # åˆ é™¤å¤šä½™çš„å¤‡ä»½æ–‡ä»¶
        for old_backup in backup_files[10:]:
            old_backup.unlink()
            logger.info(f"å·²åˆ é™¤æ—§å¤‡ä»½: {old_backup}")
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] ç³»ç»Ÿè‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œæ¢å¤
- [ ] æ•°æ®å¤‡ä»½å’Œæ¢å¤æœºåˆ¶å®Œæ•´
- [ ] ç›‘æ§æŒ‡æ ‡è¦†ç›–æ‰€æœ‰å…³é”®ç»„ä»¶

## ğŸ› ï¸ å¼€å‘ç¯å¢ƒè®¾ç½®

### æœ¬åœ°å¼€å‘ç¯å¢ƒ
```bash
# 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
cd /home/ragsvr/projects/ragsystem/web-api
python -m venv venv
source venv/bin/activate

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘.envæ–‡ä»¶ï¼Œè®¾ç½®å¿…è¦çš„é…ç½®

# 4. å¯åŠ¨å¼€å‘æœåŠ¡å™¨
python main.py

# 5. APIæ–‡æ¡£è®¿é—®
http://localhost:8001/docs
```

### è°ƒè¯•å’Œæµ‹è¯•
```bash
# 1. è¿è¡Œå•å…ƒæµ‹è¯•
pytest tests/

# 2. APIæµ‹è¯•
curl -X GET http://localhost:8001/health
curl -X POST http://localhost:8001/query -H "Content-Type: application/json" -d '{"query": "test"}'

# 3. æ€§èƒ½æµ‹è¯•
ab -n 100 -c 10 http://localhost:8001/health
```

## ğŸ“Š è´¨é‡æ ‡å‡†

### æ€§èƒ½æŒ‡æ ‡
- **APIå“åº”æ—¶é—´**: 95%è¯·æ±‚<500ms
- **æŸ¥è¯¢å¤„ç†æ—¶é—´**: å¹³å‡<3ç§’
- **æ–‡æ¡£å¤„ç†å¹¶å‘**: æ”¯æŒ4ä¸ªæ–‡æ¡£åŒæ—¶å¤„ç†
- **ç³»ç»Ÿååé‡**: >100 QPS

### ç¨³å®šæ€§æŒ‡æ ‡
- **ç³»ç»Ÿå¯ç”¨æ€§**: 99.5%ä»¥ä¸Š
- **é”™è¯¯ç‡**: <0.1%
- **å†…å­˜æ³„æ¼**: é•¿æœŸè¿è¡Œå†…å­˜ç¨³å®š
- **æ•…éšœæ¢å¤æ—¶é—´**: <30ç§’

### å®‰å…¨æ ‡å‡†
- **è¾“å…¥éªŒè¯**: 100%è¦†ç›–
- **SQLæ³¨å…¥é˜²æŠ¤**: å®Œå…¨é˜²æŠ¤
- **æ–‡ä»¶ä¸Šä¼ å®‰å…¨**: å®Œæ•´éªŒè¯
- **APIé¢‘ç‡é™åˆ¶**: åˆç†è®¾ç½®

## ğŸš€ éƒ¨ç½²é…ç½®

### å¼€å‘ç¯å¢ƒéƒ¨ç½²
```bash
# 1. å¯åŠ¨å¼€å‘æœåŠ¡å™¨
export ENVIRONMENT=development
python main.py --host 0.0.0.0 --port 8001

# 2. å¯ç”¨è°ƒè¯•æ¨¡å¼
export DEBUG=true
export LOG_LEVEL=DEBUG
```

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
```bash
# 1. ä½¿ç”¨Gunicornéƒ¨ç½²
pip install gunicorn uvicorn[standard]
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8001

# 2. ä½¿ç”¨Dockeréƒ¨ç½²
docker build -t rag-anything-api .
docker run -d -p 8001:8001 --name rag-api rag-anything-api

# 3. ä½¿ç”¨systemdæœåŠ¡
sudo cp rag-anything-api.service /etc/systemd/system/
sudo systemctl enable rag-anything-api
sudo systemctl start rag-anything-api
```

### ç›‘æ§éƒ¨ç½²
```bash
# 1. Prometheusé…ç½®
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'rag-anything-api'
    static_configs:
      - targets: ['localhost:8001']

# 2. å¯åŠ¨ç›‘æ§
docker run -d -p 9090:9090 -v $(pwd)/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus
```

## ğŸ“ æµ‹è¯•è®¡åˆ’

### å•å…ƒæµ‹è¯•
```python
# tests/test_api.py
import pytest
from fastapi.testclient import TestClient
from main import app

client = TestClient(app)

def test_health_endpoint():
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "healthy"

def test_query_endpoint():
    response = client.post("/query", json={"query": "test", "mode": "hybrid"})
    assert response.status_code == 200
    assert "result" in response.json()

def test_document_upload():
    with open("test_file.pdf", "rb") as f:
        response = client.post("/documents/upload", files={"file": f})
    assert response.status_code == 200
```

### é›†æˆæµ‹è¯•
```python
# tests/test_integration.py
@pytest.mark.asyncio
async def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹"""
    # 1. ä¸Šä¼ æ–‡æ¡£
    upload_response = await upload_test_document()
    assert upload_response["status"] == "success"
    
    # 2. ç­‰å¾…å¤„ç†å®Œæˆ
    await wait_for_processing_complete(upload_response["task_id"])
    
    # 3. æŸ¥è¯¢æ–‡æ¡£å†…å®¹
    query_response = await query_document_content()
    assert query_response["confidence_score"] > 0.8
```

### å‹åŠ›æµ‹è¯•
```python
# tests/test_load.py
import asyncio
import aiohttp

async def test_concurrent_queries():
    """æµ‹è¯•å¹¶å‘æŸ¥è¯¢æ€§èƒ½"""
    async with aiohttp.ClientSession() as session:
        tasks = []
        for i in range(100):
            task = session.post("/query", json={"query": f"test query {i}"})
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks)
        success_count = sum(1 for r in responses if r.status == 200)
        assert success_count >= 95  # 95%æˆåŠŸç‡
```

## ğŸ“‹ éªŒæ”¶æ¸…å•

### Phase 1éªŒæ”¶
- [ ] æ‰€æœ‰APIæ¥å£è¿”å›æ­£ç¡®æ ¼å¼
- [ ] ä¸LightRAG WebUIå®Œå…¨å…¼å®¹
- [ ] åŸºç¡€CRUDåŠŸèƒ½æ­£å¸¸
- [ ] é”™è¯¯å¤„ç†æœºåˆ¶å®Œå–„

### Phase 2éªŒæ”¶
- [ ] å¼‚å¸¸æƒ…å†µå¤„ç†å®Œæ•´
- [ ] è¾“å…¥éªŒè¯å’Œå®‰å…¨é˜²æŠ¤åˆ°ä½
- [ ] æ—¥å¿—è®°å½•è¯¦ç»†å‡†ç¡®
- [ ] æ€§èƒ½åœ¨è´Ÿè½½ä¸‹ç¨³å®š

### Phase 3éªŒæ”¶
- [ ] å¹¶å‘å¤„ç†èƒ½åŠ›æå‡æ˜æ˜¾
- [ ] ç¼“å­˜å‘½ä¸­ç‡è¾¾åˆ°é¢„æœŸ
- [ ] å­˜å‚¨æ€§èƒ½ä¼˜åŒ–æœ‰æ•ˆ
- [ ] èµ„æºä½¿ç”¨æ•ˆç‡æå‡

### æœ€ç»ˆéªŒæ”¶
- [ ] æ‰€æœ‰åŠŸèƒ½ç¨³å®šå¯é 
- [ ] æ€§èƒ½æŒ‡æ ‡è¾¾åˆ°è¦æ±‚
- [ ] ç›‘æ§å’Œå‘Šè­¦å®Œæ•´
- [ ] ç”Ÿäº§éƒ¨ç½²å°±ç»ª

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-08-17
**è´Ÿè´£äºº**: åç«¯å¼€å‘å›¢é˜Ÿ
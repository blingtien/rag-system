#!/usr/bin/env python
"""
ç®€å•çš„è‡ªå®šä¹‰æ¨¡å‹ç¤ºä¾‹ - Qwenæœ¬åœ°embedding + DeepSeek API

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºå¦‚ä½•ï¼š
1. ä½¿ç”¨æœ¬åœ°Qwen embeddingæ¨¡å‹
2. ä½¿ç”¨DeepSeek APIä½œä¸ºLLM
3. æ­£ç¡®é…ç½®RAGAnything
"""

import asyncio
import os
from pathlib import Path

# ç¡®ä¿èƒ½å¯¼å…¥æ¨¡å—
import sys
sys.path.append(str(Path(__file__).parent.parent))

from dotenv import load_dotenv
from lightrag.utils import EmbeddingFunc

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹å‡½æ•°
from raganything.custom_models import (
    get_qwen_embedding_func,
    deepseek_complete_if_cache,
    deepseek_vision_complete,
    validate_custom_models
)
from raganything import RAGAnything, RAGAnythingConfig

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def create_custom_llm_func():
    """åˆ›å»ºDeepSeek LLMå‡½æ•°"""
    def llm_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return deepseek_complete_if_cache(
            model=os.getenv("DEEPSEEK_MODEL", "deepseek-chat"),
            prompt=prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            temperature=float(os.getenv("TEMPERATURE", "0.0")),
            max_tokens=int(os.getenv("MAX_TOKENS", "4096")),
            **kwargs
        )
    return llm_func

def create_custom_vision_func():
    """åˆ›å»ºDeepSeekè§†è§‰æ¨¡å‹å‡½æ•°"""
    def vision_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):
        # å¦‚æœæœ‰é¢„æ ¼å¼åŒ–çš„messagesï¼Œç›´æ¥ä½¿ç”¨
        if messages:
            return deepseek_complete_if_cache(
                model=os.getenv("DEEPSEEK_VISION_MODEL", "deepseek-vl"),
                prompt="",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                **kwargs
            )
        # å¦‚æœæœ‰å›¾åƒæ•°æ®ï¼Œä½¿ç”¨è§†è§‰å®Œæˆ
        elif image_data:
            return deepseek_vision_complete(
                model=os.getenv("DEEPSEEK_VISION_MODEL", "deepseek-vl"),
                prompt=prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                image_data=image_data,
                **kwargs
            )
        # å¦åˆ™ä½¿ç”¨æ™®é€šæ–‡æœ¬å®Œæˆ
        else:
            return deepseek_complete_if_cache(
                model=os.getenv("DEEPSEEK_MODEL", "deepseek-chat"),
                prompt=prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                **kwargs
            )
    return vision_func

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è‡ªå®šä¹‰æ¨¡å‹é…ç½®...")
    
    # 1. éªŒè¯é…ç½®
    print("\nğŸ“‹ éªŒè¯ä¾èµ–å’Œé…ç½®...")
    validation = validate_custom_models()
    for key, value in validation.items():
        status = "âœ…" if value else "âŒ"
        print(f"  {status} {key}: {value}")
    
    if not validation["torch_available"]:
        print("âŒ PyTorchæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install torch")
        return
    
    if not validation["transformers_available"]:
        print("âŒ Transformersæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install transformers")
        return
    
    # 2. åˆ›å»ºåµŒå…¥å‡½æ•°
    print("\nğŸ§  åˆå§‹åŒ–Qwen embeddingæ¨¡å‹...")
    try:
        embedding_func = get_qwen_embedding_func()
        print("âœ… Qwen embeddingæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•embedding
        test_embeddings = embedding_func(["æµ‹è¯•æ–‡æœ¬"])
        print(f"âœ… Embeddingæµ‹è¯•æˆåŠŸï¼Œç»´åº¦: {len(test_embeddings[0])}")
        
    except Exception as e:
        print(f"âŒ Qwen embeddingæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # 3. åˆ›å»ºLLMå‡½æ•°
    print("\nğŸ¤– é…ç½®DeepSeek LLM...")
    if not os.getenv("DEEPSEEK_API_KEY"):
        print("âš ï¸  DEEPSEEK_API_KEYæœªè®¾ç½®ï¼Œè¯·åœ¨.envä¸­é…ç½®")
        print("   ç¤ºä¾‹: DEEPSEEK_API_KEY=your_api_key_here")
        return
    
    llm_func = create_custom_llm_func()
    vision_func = create_custom_vision_func()
    print("âœ… DeepSeek APIé…ç½®å®Œæˆ")
    
    # 4. é…ç½®RAGAnything
    print("\nâš™ï¸  åˆå§‹åŒ–RAGAnything...")
    config = RAGAnythingConfig(
        working_dir=os.getenv("WORKING_DIR", "./rag_storage"),
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )
    
    # åˆ›å»ºEmbeddingFuncå¯¹è±¡
    from raganything.custom_models import _qwen_model_instance
    embedding_dim = _qwen_model_instance.embedding_dim if _qwen_model_instance else 768
    
    embedding_func_obj = EmbeddingFunc(
        embedding_dim=embedding_dim,
        max_token_size=int(os.getenv("EMBEDDING_MAX_LENGTH", "512")),
        func=embedding_func
    )
    
    # åˆå§‹åŒ–RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_func,
        vision_model_func=vision_func,
        embedding_func=embedding_func_obj
    )
    
    print("âœ… RAGAnythingåˆå§‹åŒ–æˆåŠŸ")
    
    # 5. æµ‹è¯•åŠŸèƒ½
    print("\nğŸ§ª æµ‹è¯•é…ç½®...")
    
    # æµ‹è¯•LLMï¼ˆå¦‚æœæœ‰API keyï¼‰
    try:
        test_response = llm_func("ä½ å¥½ï¼Œè¯·ç®€çŸ­å›å¤")
        print(f"âœ… LLMæµ‹è¯•æˆåŠŸ: {test_response[:50]}...")
    except Exception as e:
        print(f"âš ï¸  LLMæµ‹è¯•å¤±è´¥ï¼ˆå¯èƒ½æ˜¯API keyé—®é¢˜ï¼‰: {e}")
    
    print("\nğŸ‰ é…ç½®å®Œæˆï¼ç°åœ¨å¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹å¤„ç†æ–‡æ¡£äº†ã€‚")
    print("\nğŸ“– ä½¿ç”¨ç¤ºä¾‹:")
    print("   # å¤„ç†æ–‡æ¡£")
    print("   await rag.process_document_complete('document.pdf')")
    print("   # æŸ¥è¯¢")
    print("   result = await rag.aquery('ä½ çš„é—®é¢˜', mode='hybrid')")

if __name__ == "__main__":
    asyncio.run(main())
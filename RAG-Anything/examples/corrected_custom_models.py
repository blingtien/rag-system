#!/usr/bin/env python
"""
ä¿®æ­£åçš„è‡ªå®šä¹‰æ¨¡å‹ç¤ºä¾‹ - éµå¾ªRAG-Anythingå®˜æ–¹æ¨¡å¼

åŸºäºContext7æ–‡æ¡£åˆ†æï¼Œæ­£ç¡®çš„é›†æˆæ–¹å¼æ˜¯ï¼š
1. RAG-Anythingåˆå§‹åŒ–æ—¶ä¼ é€’å‡½æ•°å‚æ•°ï¼Œä¸ä¾èµ–è‡ªå®šä¹‰ç¯å¢ƒå˜é‡
2. ç¯å¢ƒå˜é‡åœ¨å‡½æ•°å†…éƒ¨è¯»å–ï¼Œè€Œä¸æ˜¯é€šè¿‡RAGAnythingé…ç½®
3. éµå¾ªå®˜æ–¹çš„lambdaå‡½æ•°æ¨¡å¼
"""

import asyncio
import os
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).parent.parent))

from dotenv import load_dotenv
from lightrag.utils import EmbeddingFunc
from raganything import RAGAnything, RAGAnythingConfig
from raganything.custom_models import (
    get_qwen_embedding_func,
    deepseek_complete_if_cache,
    validate_custom_models
)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

async def main():
    """ä¸»å‡½æ•° - éµå¾ªå®˜æ–¹RAG-Anythingæ¨¡å¼"""
    print("ğŸš€ ä½¿ç”¨ä¿®æ­£åçš„è‡ªå®šä¹‰æ¨¡å‹é…ç½®...")
    
    # 1. éªŒè¯ä¾èµ–
    print("\nğŸ“‹ éªŒè¯ä¾èµ–...")
    validation = validate_custom_models()
    for key, value in validation.items():
        status = "âœ…" if value else "âŒ"
        print(f"  {status} {key}")
    
    if not all([validation["torch_available"], validation["transformers_available"]]):
        print("âŒ ç¼ºå°‘å¿…è¦ä¾èµ–ï¼Œè¯·å…ˆå®‰è£… torch å’Œ transformers")
        return
    
    # 2. åˆ›å»ºembeddingå‡½æ•° (éµå¾ªå®˜æ–¹lambdaæ¨¡å¼)
    print("\nğŸ§  åˆ›å»ºembeddingå‡½æ•°...")
    qwen_embedding_func = get_qwen_embedding_func()
    
    # è·å–embeddingç»´åº¦
    from raganything.custom_models import _qwen_model_instance
    embedding_dim = _qwen_model_instance.embedding_dim if _qwen_model_instance else 768
    
    # åˆ›å»ºå…¼å®¹LightRAGçš„embeddingå‡½æ•°
    embedding_func = EmbeddingFunc(
        embedding_dim=embedding_dim,
        max_token_size=int(os.getenv("EMBEDDING_MAX_LENGTH", "512")),
        func=qwen_embedding_func
    )
    print(f"âœ… Qwen embeddingé…ç½®å®Œæˆï¼Œç»´åº¦: {embedding_dim}")
    
    # 3. åˆ›å»ºLLMå‡½æ•° (éµå¾ªå®˜æ–¹lambdaæ¨¡å¼)
    print("\nğŸ¤– åˆ›å»ºLLMå‡½æ•°...")
    
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        """LLMå‡½æ•° - éµå¾ªå®˜æ–¹openai_complete_if_cacheç­¾å"""
        return deepseek_complete_if_cache(
            model=os.getenv("DEEPSEEK_MODEL", "deepseek-chat"),
            prompt=prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            temperature=float(os.getenv("TEMPERATURE", "0.0")),
            max_tokens=int(os.getenv("MAX_TOKENS", "4096")),
            **kwargs
        )
    
    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):
        """è§†è§‰æ¨¡å‹å‡½æ•° - éµå¾ªå®˜æ–¹æ¨¡å¼"""
        # å¦‚æœæœ‰é¢„æ ¼å¼åŒ–çš„messagesï¼Œç›´æ¥ä½¿ç”¨
        if messages:
            return deepseek_complete_if_cache(
                model=os.getenv("DEEPSEEK_VISION_MODEL", "deepseek-vl"),
                prompt="",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                **kwargs
            )
        # å¦‚æœæœ‰å›¾åƒæ•°æ®
        elif image_data:
            # æ„é€ è§†è§‰æ¶ˆæ¯æ ¼å¼
            content = [{"type": "text", "text": prompt}]
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
            })
            
            vision_messages = []
            if system_prompt:
                vision_messages.append({"role": "system", "content": system_prompt})
            if history_messages:
                vision_messages.extend(history_messages)
            vision_messages.append({"role": "user", "content": content})
            
            return deepseek_complete_if_cache(
                model=os.getenv("DEEPSEEK_VISION_MODEL", "deepseek-vl"),
                prompt="",
                system_prompt=None,
                history_messages=[],
                messages=vision_messages,
                **kwargs
            )
        else:
            # å›é€€åˆ°æ–‡æœ¬æ¨¡å¼
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)
    
    print("âœ… DeepSeek LLMå‡½æ•°é…ç½®å®Œæˆ")
    
    # 4. æŒ‰å®˜æ–¹æ¨¡å¼åˆå§‹åŒ–RAGAnything
    print("\nâš™ï¸  åˆå§‹åŒ–RAGAnything...")
    
    # ä½¿ç”¨å®˜æ–¹æ”¯æŒçš„é…ç½®å‚æ•°
    config = RAGAnythingConfig(
        working_dir=os.getenv("WORKING_DIR", "./rag_storage"),
        parser_output_dir=os.getenv("OUTPUT_DIR", "./output"),
        parser=os.getenv("PARSER", "mineru"),
        parse_method=os.getenv("PARSE_METHOD", "auto"),
        enable_image_processing=os.getenv("ENABLE_IMAGE_PROCESSING", "true").lower() == "true",
        enable_table_processing=os.getenv("ENABLE_TABLE_PROCESSING", "true").lower() == "true",
        enable_equation_processing=os.getenv("ENABLE_EQUATION_PROCESSING", "true").lower() == "true",
    )
    
    # æŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„æ–¹å¼åˆå§‹åŒ–
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )
    
    print("âœ… RAGAnythingåˆå§‹åŒ–æˆåŠŸ")
    
    # 5. æµ‹è¯•é…ç½®
    print("\nğŸ§ª æµ‹è¯•é…ç½®...")
    
    # æµ‹è¯•embedding
    try:
        test_embeddings = qwen_embedding_func(["æµ‹è¯•embedding"])
        print(f"âœ… Embeddingæµ‹è¯•æˆåŠŸ: {len(test_embeddings[0])}ç»´")
    except Exception as e:
        print(f"âŒ Embeddingæµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•LLMï¼ˆéœ€è¦API keyï¼‰
    if os.getenv("DEEPSEEK_API_KEY", "").startswith("sk-"):
        try:
            test_response = llm_model_func("ç®€çŸ­å›å¤ï¼šä½ å¥½")
            print(f"âœ… LLMæµ‹è¯•æˆåŠŸ: {test_response[:50]}...")
        except Exception as e:
            print(f"âš ï¸  LLMæµ‹è¯•å¤±è´¥: {e}")
    else:
        print("âš ï¸  DEEPSEEK_API_KEYæœªè®¾ç½®ï¼Œè·³è¿‡LLMæµ‹è¯•")
    
    print("\nğŸ‰ é…ç½®éªŒè¯å®Œæˆï¼")
    print("\nğŸ“– ä½¿ç”¨æ–¹æ³•:")
    print("   # å¤„ç†æ–‡æ¡£")
    print("   await rag.process_document_complete('document.pdf')")
    print("   # æŸ¥è¯¢æ–‡æ¡£")
    print("   result = await rag.aquery('é—®é¢˜', mode='hybrid')")
    
    return rag

if __name__ == "__main__":
    asyncio.run(main())
#!/usr/bin/env python3
"""
æ–‡æ¡£é‡å¤é—®é¢˜ä¸€é”®ä¿®å¤è„šæœ¬
é›†æˆæ‰€æœ‰ä¿®å¤å·¥å…·ï¼Œæä¾›ç®€å•çš„å‘½ä»¤è¡Œç•Œé¢
"""

import os
import sys
import json
import logging
import asyncio
from datetime import datetime
from pathlib import Path

# æ·»åŠ APIè·¯å¾„
sys.path.append(str(Path(__file__).parent / "RAG-Anything" / "api"))

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('document_repair.log')
        ]
    )
    return logging.getLogger(__name__)

def main():
    """ä¸»ä¿®å¤æµç¨‹"""
    logger = setup_logging()
    
    print("=" * 60)
    print("ğŸ› ï¸  RAGæ–‡æ¡£é‡å¤é—®é¢˜ä¿®å¤å·¥å…·")
    print("=" * 60)
    
    try:
        # å¯¼å…¥ä¿®å¤å·¥å…·
        from utils.document_deduplicator import DocumentDeduplicator
        from utils.storage_consistency_repair import StorageConsistencyRepairer
        from utils.atomic_document_processor import AtomicDocumentProcessor
        
        storage_dir = "./rag_storage"
        
        print(f"\nğŸ“ å·¥ä½œç›®å½•: {storage_dir}")
        print(f"ğŸ“… æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ­¥éª¤1: åˆ†æå½“å‰çŠ¶æ€
        print("\nğŸ” æ­¥éª¤1: åˆ†æå½“å‰å­˜å‚¨çŠ¶æ€...")
        repairer = StorageConsistencyRepairer(storage_dir)
        analysis = repairer.analyze_consistency()
        
        print(f"   APIæ–‡æ¡£æ•°: {analysis['file_counts']['api_documents']}")
        print(f"   çŠ¶æ€è®°å½•æ•°: {analysis['file_counts']['doc_status']}")
        print(f"   å®Œæ•´æ–‡æ¡£æ•°: {analysis['file_counts']['full_docs']}")
        
        total_issues = sum(len(issues) for issues in analysis['inconsistencies'].values() if issues)
        print(f"   å‘ç°é—®é¢˜æ•°: {total_issues}")
        
        if total_issues == 0:
            print("âœ… æœªå‘ç°å­˜å‚¨ä¸€è‡´æ€§é—®é¢˜ï¼")
        else:
            print("âš ï¸  å‘ç°å­˜å‚¨ä¸ä¸€è‡´æ€§é—®é¢˜:")
            for issue_type, issues in analysis['inconsistencies'].items():
                if issues:
                    print(f"     {issue_type}: {len(issues)} é¡¹")
        
        # æ­¥éª¤2: æ£€æŸ¥é‡å¤æ–‡æ¡£
        print("\nğŸ” æ­¥éª¤2: æ£€æŸ¥é‡å¤æ–‡æ¡£...")
        deduplicator = DocumentDeduplicator(storage_dir)
        
        # ä¸ºç°æœ‰æ–‡æ¡£æ·»åŠ å“ˆå¸Œ
        hash_result = deduplicator.add_hash_to_documents(dry_run=False)
        print(f"   å¤„ç†æ–‡æ¡£: {hash_result['processed_documents']}")
        print(f"   æ·»åŠ å“ˆå¸Œ: {hash_result['successful_hashes']}")
        print(f"   å·²æœ‰å“ˆå¸Œ: {hash_result['already_have_hash']}")
        
        # æŸ¥æ‰¾é‡å¤æ–‡æ¡£
        content_duplicates = deduplicator.find_duplicates_by_content()
        filename_duplicates = deduplicator.find_duplicates_by_filename()
        
        print(f"   å†…å®¹é‡å¤ç»„: {len(content_duplicates)}")
        print(f"   æ–‡ä»¶åé‡å¤ç»„: {len(filename_duplicates)}")
        
        total_duplicate_docs = sum(len(docs) - 1 for docs in content_duplicates.values())
        print(f"   å¯ç§»é™¤é‡å¤æ–‡æ¡£: {total_duplicate_docs}")
        
        # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
        if total_issues > 0 or total_duplicate_docs > 0:
            print(f"\nâš ï¸  å‘ç°é—®é¢˜:")
            print(f"   - å­˜å‚¨ä¸ä¸€è‡´æ€§: {total_issues} é¡¹")
            print(f"   - é‡å¤æ–‡æ¡£: {total_duplicate_docs} ä¸ª")
            
            response = input("\næ˜¯å¦ç»§ç»­ä¿®å¤è¿™äº›é—®é¢˜? (y/N): ").lower().strip()
            if response != 'y':
                print("âŒ ç”¨æˆ·å–æ¶ˆï¼Œé€€å‡ºä¿®å¤")
                return
        
        # æ­¥éª¤3: ä¿®å¤å­˜å‚¨ä¸€è‡´æ€§
        if total_issues > 0:
            print("\nğŸ”§ æ­¥éª¤3: ä¿®å¤å­˜å‚¨ä¸€è‡´æ€§...")
            repair_result = repairer.full_consistency_repair(dry_run=False)
            
            steps = repair_result.get('steps', {})
            print(f"   ä¿®å¤rag_doc_id: {steps.get('repair_missing_rag_ids', {}).get('repaired', 0)}")
            print(f"   æ¸…ç†å­¤ç«‹æ¡ç›®: {steps.get('clean_orphaned_entries', {}).get('total_cleaned', 0)}")
            print(f"   åŒæ­¥çŠ¶æ€: {steps.get('synchronize_status_docs', {}).get('total_synced', 0)}")
            print(f"   æ ‡å‡†åŒ–è·¯å¾„: {steps.get('standardize_paths', {}).get('standardized', 0)}")
        
        # æ­¥éª¤4: ç§»é™¤é‡å¤æ–‡æ¡£
        if total_duplicate_docs > 0:
            print("\nğŸ—‘ï¸  æ­¥éª¤4: ç§»é™¤é‡å¤æ–‡æ¡£...")
            removal_result = deduplicator.remove_duplicate_documents(content_duplicates, dry_run=False)
            
            print(f"   ç§»é™¤é‡å¤æ–‡æ¡£: {removal_result['total_documents_removed']}")
            print(f"   ä¿ç•™æœ€ä½³æ–‡æ¡£: {len(removal_result['kept_documents'])}")
            
            # æ˜¾ç¤ºç§»é™¤çš„æ–‡æ¡£è¯¦æƒ…
            if removal_result['removed_documents']:
                print("   ç§»é™¤çš„æ–‡æ¡£:")
                for doc in removal_result['removed_documents'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"     - {doc['filename']} ({doc['status']})")
                if len(removal_result['removed_documents']) > 5:
                    print(f"     ... è¿˜æœ‰ {len(removal_result['removed_documents']) - 5} ä¸ª")
        
        # æ­¥éª¤5: ç”Ÿæˆä¿®å¤æŠ¥å‘Š
        print("\nğŸ“‹ æ­¥éª¤5: ç”Ÿæˆä¿®å¤æŠ¥å‘Š...")
        
        final_analysis = repairer.analyze_consistency()
        final_duplicates = deduplicator.find_duplicates_by_content()
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'repair_summary': {
                'initial_issues': total_issues,
                'initial_duplicates': total_duplicate_docs,
                'final_issues': sum(len(issues) for issues in final_analysis['inconsistencies'].values() if issues),
                'final_duplicates': sum(len(docs) - 1 for docs in final_duplicates.values()),
                'consistency_repair': repair_result if total_issues > 0 else None,
                'duplicate_removal': removal_result if total_duplicate_docs > 0 else None
            },
            'final_state': {
                'api_documents': final_analysis['file_counts']['api_documents'],
                'doc_status': final_analysis['file_counts']['doc_status'],
                'full_docs': final_analysis['file_counts']['full_docs']
            }
        }
        
        report_file = f"document_repair_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"   ä¿®å¤æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # æœ€ç»ˆçŠ¶æ€
        print("\nâœ… ä¿®å¤å®Œæˆ!")
        print(f"   æœ€ç»ˆAPIæ–‡æ¡£æ•°: {final_analysis['file_counts']['api_documents']}")
        print(f"   æœ€ç»ˆçŠ¶æ€è®°å½•æ•°: {final_analysis['file_counts']['doc_status']}")
        print(f"   æœ€ç»ˆå®Œæ•´æ–‡æ¡£æ•°: {final_analysis['file_counts']['full_docs']}")
        print(f"   å‰©ä½™é—®é¢˜æ•°: {sum(len(issues) for issues in final_analysis['inconsistencies'].values() if issues)}")
        print(f"   å‰©ä½™é‡å¤æ–‡æ¡£: {sum(len(docs) - 1 for docs in final_duplicates.values())}")
        
        if sum(len(issues) for issues in final_analysis['inconsistencies'].values() if issues) == 0:
            print("ğŸ‰ æ‰€æœ‰å­˜å‚¨ä¸€è‡´æ€§é—®é¢˜å·²è§£å†³!")
        
        if sum(len(docs) - 1 for docs in final_duplicates.values()) == 0:
            print("ğŸ‰ æ‰€æœ‰é‡å¤æ–‡æ¡£å·²æ¸…ç†!")
        
        print("\nğŸ“ å»ºè®®:")
        print("   1. é‡å¯APIæœåŠ¡å™¨ä»¥ç¡®ä¿æ›´æ”¹ç”Ÿæ•ˆ")
        print("   2. å®šæœŸè¿è¡Œæ­¤å·¥å…·æ£€æŸ¥å’Œä¿®å¤é—®é¢˜")
        print("   3. æŸ¥çœ‹ä¿®å¤æ—¥å¿—äº†è§£è¯¦ç»†ä¿¡æ¯")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥ä¿®å¤å·¥å…·å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿åœ¨RAG-Anythingé¡¹ç›®ç›®å½•ä¸­è¿è¡Œæ­¤è„šæœ¬")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ä¿®å¤è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
        print("è¯·æŸ¥çœ‹ document_repair.log äº†è§£è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        sys.exit(1)

if __name__ == "__main__":
    main()